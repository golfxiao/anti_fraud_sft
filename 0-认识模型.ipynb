{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb606a3f-d0c9-43c1-a5bf-a3d6851e308a",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "本文主要内容如下：\n",
    "1. 从使用的角度来接触模型\n",
    "2. 本地运行的方式来认识模型\n",
    "3. 以文本生成过程来理解模型\n",
    "4. 以内部窥探的方式来解剖模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c23ad-cb10-454c-bba5-54c628b8079e",
   "metadata": {},
   "source": [
    "## 1. 模型前台使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd40c0a-888a-4b8a-9bce-60abd585f02c",
   "metadata": {},
   "source": [
    "#### 可视化UI的方式使用模型\n",
    "\n",
    "[点此进入](http://192.168.31.200:8501/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240efbf2-e4bd-47df-8f9e-1ac9fc467521",
   "metadata": {},
   "source": [
    "#### 使用openai库来访问模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b586705e-0819-4e13-af78-cb9a7eaf4105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是阿里云研发的超大规模语言模型，我叫通义千问。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def predict(prompt):\n",
    "    client = OpenAI(api_key=\"0\",base_url=\"http://192.168.31.200:8000/v1\")\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    result = client.chat.completions.create(messages=messages, model=\"Qwen2-0.5B-Instruct\")\n",
    "    return result.choices[0].message.content\n",
    "\n",
    "predict(\"详细介绍下你自己。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9559dec-deb5-454b-8192-f32eda5943c6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "上面的`client.chat.completions.create`方法调用本质上是发送了类似下面一个请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7b1ef7e-593e-4f28-9032-c0d701278231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"chat-373150f974fd4367b53632817ed44cbc\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"object\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"chat.completion\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"created\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1730414253\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"model\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Qwen2-0.5B-Instruct\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"choices\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "    \u001b[1;39m{\n",
      "      \u001b[0m\u001b[34;1m\"index\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"message\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "        \u001b[0m\u001b[34;1m\"role\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"assistant\"\u001b[0m\u001b[1;39m,\n",
      "        \u001b[0m\u001b[34;1m\"content\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"我是阿里云开发的一款超大规模语言模型，我叫通义千问。\"\u001b[0m\u001b[1;39m,\n",
      "        \u001b[0m\u001b[34;1m\"tool_calls\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m\n",
      "      \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"logprobs\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;30mnull\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"finish_reason\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"stop\"\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"stop_reason\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;30mnull\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"usage\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"prompt_tokens\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m23\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"total_tokens\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m41\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"completion_tokens\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m18\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"prompt_logprobs\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;30mnull\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!curl -s http://192.168.31.200:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{ \\\n",
    "        \"model\": \"Qwen2-0.5B-Instruct\",\\\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"详细介绍下你自己。\"}],\\\n",
    "        \"max_tokens\": 512,\\\n",
    "        \"temperature\": 0.7  \\\n",
    "    }' | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056c1db-8cac-41c8-a905-927cb3c62f50",
   "metadata": {},
   "source": [
    "> 上面是OpenAI接口的完整response，在聊天窗口中显示的推理结果只是取了`response['choices'][0]['message']['content']`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420dcdbf-7d51-405a-b33b-f544c5048b7a",
   "metadata": {},
   "source": [
    "## 2. 模型后台运行\n",
    "\n",
    "模型的后台运行我们可以简单分为如下四步：\n",
    "1. 标准化提示语\n",
    "2. 序列化提示语\n",
    "3. 模型预测\n",
    "4. 反序列化为文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14ce3b-6d80-4a18-82cf-f6328d294b9b",
   "metadata": {},
   "source": [
    "#### 2.1 加载模型\n",
    "加载模型最基本的方式是使用 `transformers` 库，它是由 Hugging Face 开发的一个开源库，它提供了很方便的方式来访问和使用各种自然语言处理（NLP）模型，也提供了便捷的API来微调和训练开源模型。\n",
    "\n",
    "> 目前业界开源的语言模型基本都会发布到HuggingFace，也都支持transformers库来加载。huggingface上不仅提供模型、开发库，还提供数据集甚至算力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b424ec8-c04d-429f-b2d5-15dfc055055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 定义模型本地路径和要加载到的目标设备\n",
    "device = \"cuda:4\" \n",
    "model_path = \"/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-0___5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c95c5893-e81c-4501-8626-7ce6c85cebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 976188\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua       659 Aug  1 18:26 config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua        48 Aug  1 18:26 configuration.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua       242 Aug  1 18:26 generation_config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua     11344 Aug  1 18:26 LICENSE\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua   1671839 Aug  1 18:26 merges.txt\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua 988097824 Aug  1 18:31 model.safetensors\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua      3551 Aug  1 18:31 README.md\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua      1287 Aug  1 18:31 tokenizer_config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua   7028015 Aug  1 18:31 tokenizer.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua   2776833 Aug  1 18:31 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-0___5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ed875b-1e91-4459-b7d0-3cf8c14b8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45744233-eb85-410f-b79d-22eb10968a57",
   "metadata": {},
   "source": [
    "- AutoModelForCausalLM是用于加载因果语言模型，这类模型在生成新词时只依赖于之前生成的词，而不考虑未来的词。\n",
    "\n",
    "- AutoTokenizer是用于文本分词的组件，它能根据模型路径自动加载词表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446c637-debb-4480-b619-e83414e1dd7d",
   "metadata": {},
   "source": [
    "#### 2.2 标准化提示语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed702a60-87a9-4ca9-a19d-1787e6b34047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "详细介绍下你自己。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"详细介绍下你自己。\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "# 将消息应用到标准模板，使结构一致\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "print(f\"{input_text}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74790f7-ac3e-4a23-9cd6-a932e539e79e",
   "metadata": {},
   "source": [
    "**疑问**：为什么不使用原先的json分隔符，而要单独定义一套标记作为分隔符呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "162fb963-7900-4b85-9105-feb3f50b33d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['con', 'gr', 'at', 'ulation']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"congratulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefb9ef-9c29-41ee-982b-730717175e5e",
   "metadata": {},
   "source": [
    "#### 2.3 提示语序列化\n",
    "序列化的本质就是把人类理解的文本转换为模型可以计算的数字。\n",
    "![tokenize.png](./img/认识模型/tokenize.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dcb234c-24e1-4641-a0b3-f7c19ca86358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 113511,  16872, 107828,   1773,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:4'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:4')}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([input_text], return_tensors=\"pt\").to(device)\n",
    "print(f\"{model_inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffc5c0-03c9-4d29-8d6f-52d77c71bf0a",
   "metadata": {},
   "source": [
    "- return_tensors='pt'表示以pytorch张量的形式返回，并通过'.to(device)'来将数据移动到指定的GPU设备上，便于后续计算。\n",
    "\n",
    "- attention_mask主要用于批量预测场景，用于计算注意力时屏蔽填充token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d233f-8add-4740-bbad-02cb14db63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780ef95-871e-44fd-8f99-aa350fb1d881",
   "metadata": {},
   "source": [
    "#### 2.4 模型预测生成\n",
    "\n",
    "`model.generate`函数用于对一个序列进行推理，并输出预测的新序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee4fd93-2ebd-4cda-ac86-b74d532713c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_ids: tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 113511,  16872, 107828,   1773,\n",
      "         151645,    198, 151644,  77091,    198, 104198, 101919, 102661,  99718,\n",
      "         104197, 100176, 102064, 104949,   3837,  35946,  99882,  31935,  64559,\n",
      "          99320,  56007,   1773, 151645]], device='cuda:4')\n"
     ]
    }
   ],
   "source": [
    "response_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "print(f\"response_ids: {response_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d55471-3a7a-4c2a-b3d5-adc55e528051",
   "metadata": {},
   "source": [
    "#### 2.5 反序列化为文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dafb4e9e-1ccf-41f9-8e18-4bfa39ea2072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_ids: [tensor([104198, 101919, 102661,  99718, 104197, 100176, 102064, 104949,   3837,\n",
      "         35946,  99882,  31935,  64559,  99320,  56007,   1773, 151645],\n",
      "       device='cuda:4')]\n",
      "我是来自阿里云的大规模语言模型，我叫通义千问。\n"
     ]
    }
   ],
   "source": [
    "response_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, response_ids)\n",
    "]\n",
    "print(f\"response_ids: {response_ids}\")\n",
    "response_text = tokenizer.batch_decode(response_ids, skip_special_tokens=True)[0]\n",
    "print(f\"{response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb686047-4ce5-42d8-a71b-dec8b0260502",
   "metadata": {},
   "source": [
    "> 模型输出的序列response_ids，是同时包含输入和输出的完整序列，需要后期处理来截掉输入序列，只保留输出序列。\n",
    "\n",
    "> 模型的输出结果中会包含特殊token（例如：<|im_end|>），需要通过skip_special_tokens=False过滤，只保留真正有效的文本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d54e0-4439-4d77-aff6-701ab93c5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e03c3-917e-41fc-87b6-91ef06c20502",
   "metadata": {},
   "source": [
    "上面是模型生成文本的一个过程。\n",
    "\n",
    "这里有一个疑问：模型是一次性生成整个序列吗？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b25b8c-b4f0-4610-a154-a4660ba3aba2",
   "metadata": {},
   "source": [
    "## 3. model.generate的运行探究\n",
    "\n",
    "实际上model.generate并不是pytorch的标准API，而是为了方便调用而高度封装过的，`model.forward`才是pytorch模型标准调用方法。\n",
    "> PyTorch 是Facebook开发一个深度学习框架，它相比于Tensorflow来说更加容易学习和上手，其发展速度已经超过了Tensorflow。\n",
    "\n",
    "#### 3.1 手动模拟\n",
    "我们尝试还原一下`model.generate`方法的运行过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2aee727-1a71-4d57-9239-80ee8d6830b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义最终的序列，初始值为输入序列\n",
    "generate_ids = model_inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86df5d-c676-43f1-9347-0c80687612be",
   "metadata": {},
   "outputs": [],
   "source": [
    "定义一个函数`generate_next_token`,作用是预测一个token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088ff68d-ec9d-4105-aa4b-d87020cb6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(model, generate_ids, temperature=0.7, debug=False):\n",
    "    print(\"input_ids:\", generate_ids) if debug else None\n",
    "    logits = model.forward(generate_ids).logits\n",
    "    print(\"logits: \", logits) if debug else None\n",
    "    \n",
    "    if temperature > 0:\n",
    "        probs = torch.softmax(logits[:, -1] / temperature, dim = -1)\n",
    "        print(\"probs: \", len(probs[0]), probs) if debug else None\n",
    "        next_token = torch.multinomial(probs[-1], num_samples=1)  # 按照概率采样得到下一个token\n",
    "    else:\n",
    "        next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "    \n",
    "    print(\"next_id: \", next_token, \", token: \", tokenizer.decode(next_token)) if debug else None\n",
    "    return next_token.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9daff7-542b-4189-84f3-fc11ffe6dd9d",
   "metadata": {},
   "source": [
    "- `model.forward`是为了执行矩阵运算，得到未归一化的概率。\n",
    "- `torch.softmax`是为了得到归一化的概率（所有可能值的概率之和为1），`logits[:, -1]`表示取三维张量的第二维最后一列，即最后一个token的张量，用于预测下一个token。\n",
    "- `temperature`在这里起到的作用是：温度越高（大于1），概率分布越平滑，温度越低（小于1），概率分布越尖锐。\n",
    "- `torch.multinomial`：按照概率大小进行随机加权采样，num_samples=1表示只采样一个值作为结果返回。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb1de0-21b5-4269-a983-c85e0d4eecbf",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a19c263-8be0-482f-884a-62eece68d4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 113511,  16872, 107828,   1773,\n",
      "         151645,    198, 151644,  77091,    198, 104198, 101919]],\n",
      "       device='cuda:4')\n",
      "logits:  tensor([[[ 2.4219,  2.6250,  2.9531,  ..., -3.3281, -3.3281, -3.3281],\n",
      "         [ 0.5938,  1.8359,  3.6250,  ..., -4.5000, -4.5000, -4.5000],\n",
      "         [ 8.0625,  4.5938, 10.8125,  ..., -3.4531, -3.4531, -3.4531],\n",
      "         ...,\n",
      "         [ 6.0312, 10.1250,  5.4062,  ..., -3.4062, -3.3906, -3.3906],\n",
      "         [ 2.5781,  6.5000,  3.3906,  ..., -5.1562, -5.1562, -5.1562],\n",
      "         [ 2.0781,  6.0938,  3.8438,  ..., -5.9062, -5.8750, -5.8750]]],\n",
      "       device='cuda:4', grad_fn=<ToCopyBackward0>)\n",
      "probs:  151936 tensor([[5.4616e-10, 1.6931e-07, 6.8037e-09,  ..., 6.0764e-15, 6.3538e-15,\n",
      "         6.3538e-15]], device='cuda:4', grad_fn=<SoftmaxBackward0>)\n",
      "next_id:  tensor([102661], device='cuda:4') , token:  阿里\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我是来自阿里'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = generate_next_token(model, generate_ids, debug=True)\n",
    "generate_ids = torch.cat((generate_ids, next_token), dim=1)\n",
    "tokenizer.batch_decode(generate_ids[:, len(model_inputs.input_ids[0]):], skip_special_tokens=True)[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f25a2b-54be-4519-84c1-8019ed84582f",
   "metadata": {},
   "source": [
    ">`torch.cat`是用于将下一个token追加到序列的末尾，作为新的上下文重新输入给模型。\n",
    "\n",
    "\n",
    "> 通过generate_next_token产生下一个token后，需要将这个token追加到输入序列的结尾，继续预测下下一个token。预测到下下一个token后，继续追加到输入序列的结尾……如此循环，来逐步生成一个完整的序列。\n",
    "\n",
    "#### 3.2 自动化\n",
    "将上面的手动一次一次运行的过程，写成一个循环，一次性输出整个序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53779f17-2957-4e25-a6ac-c8158a0478e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Tokenizer(name_or_path='/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-0___5B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2214fc9-dc3d-437c-a0a0-2f4da60c5a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作为一个人工智能，我是一个计算机程序，没有情感、意识或身体，因此我无法有自己的思想、感情或身体。我被设计为帮助用户解答问题、提供信息和完成任务。我每天都会不断学习和更新，以便更好地服务用户。"
     ]
    }
   ],
   "source": [
    "import time\n",
    "max_new_tokens = 128\n",
    "end_token_id = 151645\n",
    "generate_ids = model_inputs.input_ids\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    next_token = generate_next_token(model, generate_ids, debug=False)\n",
    "    generate_ids = torch.cat((generate_ids, next_token), dim=1)\n",
    "    if next_token.item() == end_token_id:\n",
    "        break\n",
    "    print(tokenizer.decode(next_token[0], skip_special_tokens=False), end='')\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617b8cc-d0a2-46b4-9769-06b44e7b06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174388e-7f9b-4234-ae78-038eb189dc32",
   "metadata": {},
   "source": [
    "疑问：模型为什么要一个token一个token生成，而不是一遍矩阵运算生成多个词呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c64b79-f6e4-4953-9cee-95d77b848698",
   "metadata": {},
   "source": [
    "#### 3.3 自回归\n",
    "\n",
    "像上面这样，每生成一个token，都追加到原上下文的末尾，作为新的上下文来预测下一个token这种生成方式，就叫`自回归解码`。\n",
    "\n",
    "> 采用自回归解码的模型被称为因果语言模型，这类模型预测下一个token只依赖前面的token，也是上面`AutoModelForCausalLM`这个名字的由来。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8ef76-d7d0-4500-8d37-e97982edeecf",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./img/认识模型/auto_regression.png\" width=\"700px\" height=\"700px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30419ea5-60be-4362-8b35-41ab77ca7b20",
   "metadata": {},
   "source": [
    "## 4. model内部是什么？\n",
    "\n",
    "#### 4.1 主体结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd0af12-c58a-4706-be0f-9a14253a4165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm()\n",
      "        (post_attention_layernorm): Qwen2RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732af80f-9951-48ee-bdfe-2371698bac8c",
   "metadata": {},
   "source": [
    "Attention+MLP参数量估算：\n",
    "- W(q)参数量 = 896 × 896 + 896 = 803712\n",
    "- W(k)参数量 = 896 × 128 + 128 = 115584\n",
    "- W(v)参数量 = 896 × 128 + 128 = 115584\n",
    "- W(o)参数量 = 896 × 896 = 802816\n",
    "- W(gate)参数量 = 896 × 4864 = 4358144\n",
    "- W(up)参数量 = 896 × 4864 = 4358144\n",
    "- W(down)参数量 = 4864 × 896 = 4358144\n",
    "\n",
    "DecoderLayer参数量估算\n",
    "- W(DecoderLayer) = W(q) + W(k) + W(v) + W(o) + W(gate) + W(up) + W(down) = 14911928\n",
    "- W(lm_head)参数量 = 896 × 151936 = 136134656\n",
    "\n",
    "模型总参数量估算：\n",
    "- W(model) = W(Decoderlayer) * 24 + W(lm_head) = 357886272 + 136134656 = 494020928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9668c4c-1a95-4a75-902a-689075afede2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5182a5a71a4bb899b7d075a8f8ee75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path_7b = \"/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-7B-Instruct\"\n",
    "model_7b = AutoModelForCausalLM.from_pretrained(model_path_7b, torch_dtype=torch.bfloat16).to(device)\n",
    "model_7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ff97a-ac43-4aff-a9f9-56346987cbdc",
   "metadata": {},
   "source": [
    "> 每生成一个token，都要进行24层的解码器运算，每层解码器都要有14911928个参数进行运算，所以语言模型的推理功能对GPU运算的消耗是巨大的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f604e3-41aa-421d-8611-1b02d98d683e",
   "metadata": {},
   "source": [
    "#### 4.2 组件架构\n",
    "模型的整个结构基本如下图所示：\n",
    "\n",
    "![framework.jpg](./img/认识模型/framework.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32adc9f-6a08-4657-a920-771ccf6850a9",
   "metadata": {},
   "source": [
    "- `hidden_states`: 隐藏状态，除了输入和输出外，中间计算结果都称为隐藏状态，它会随着每层解码器的运算而不断更新，更新后再作为下一层解码器的输入，原始值为embedding后的输入向量，经过所有解码器运算完后作为最终输出。\n",
    "- `RMSNorm`: 是一种归一化技术，它能将序列中的所有值都规范化到同一尺度，以稳定模型的训练过程。\n",
    "- `residual`: 残差连接，通过在神经网络的某些层之间添加跨层连接，将输入信号直接传递到后续层，目的是为了缓解训练过程中的梯度消失问题，并学习数据之间的长期依赖关系，它假设当前的输出与之前或更以前的输入序列之间存在着依赖关系。\n",
    "- `MLP`: 多层感知器，是一个前馈神经网络（Feed forward network)，通过维度放大来增加信息的丰富程度，再通过维度缩小来对提取的特征进行压缩以捕捉到不同层次的抽象特征，并通过激活函数引入非线性变换来提高模型的表达能力。\n",
    "- `Attention`: 注意力机制，目的是为了获得每个单词在一个句子(上下文)中的含义，它的作法是计算`每个词与序列中其它词之间的相关度得分`（即注意力），它假设人类是`通过句子中某几个关键词来理解一句话中某个词的含义`。\n",
    "\n",
    "\n",
    "\n",
    "两个例句对比：\n",
    "1. 明天 早点 来。   --> 时间\n",
    "2. 你吃早点了吗？ --> 早餐\n",
    "\n",
    "自注意力中的Q、K、V理解：\n",
    "- Q(Query)：表示的是查询，可以理解当前每个输入位置的信息本身，以前面的句子为例，要衡量`早点`对`来`的注意力时，`早点`就是Q； \n",
    "- K(Key)：表示输入序列中每个位置的索引，即`来`这个词所在的位置； \n",
    "- V(Value)：表示与每个位置（K）相关联的内容，它能用来表示实际要被关注并对后续处理有价值的信息； \n",
    "- K和Q的点积表示词元与词元之间两两注意力权重，它决定了哪些输入位置的Value信息对当前Query最重要，假如`来`对`早点`这个token最重要，那`早点`和`来`的点积就会比较大；\n",
    "- AttentionWeight(注意力权重)再与V加权求和，就得到一个充分理解了每个单词上下文含义的新向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8c0cf-93f2-4798-97bc-c31b18954aef",
   "metadata": {},
   "source": [
    "疑问：模型不应该是编码器——解码器架构吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c083e-5fda-4253-b68f-9d09b2ca5d0c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./img/认识模型/transformer.png\" width=\"450px\" height=\"450px\"/>\n",
    "\n",
    "除了结构比仅解码器架构更复杂外，编码器-解码器架构的最大的局限性在于，解码器在解码时只能依赖于编码器输出的最后隐藏状态，该架构期望的是编码器输出的隐藏状态能完全表示输入序列的所有信息。\n",
    "\n",
    "但实际上这个很难做到，当这个隐藏状态信息不足以表示整个输入序列时，而解码器又无法直接访问原始序列和编码器中更早期的隐藏状态，最终就会导致上下文丢失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b73eab-14d1-46ec-96d9-f7fea5a8721e",
   "metadata": {},
   "source": [
    "transformer分为三种架构：\n",
    "- encoder-decoder\n",
    "- encoder-only\n",
    "- decoder-only\n",
    "\n",
    "<img src=\"./img/认识模型/llm_history.png\" width=\"900px\" height=\"900px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40b962-6d1b-4a59-8924-fdabc202ef16",
   "metadata": {},
   "source": [
    "#### 4.3 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "956b08ef-1f5e-4a0f-ade6-87e8471015c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6fda3f-bbd6-45b2-bff6-c3ec26031222",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.state_dict()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8c76702-4e90-4303-830d-ca79431bfc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0024, -0.0156,  0.0229,  ..., -0.0079, -0.0128,  0.0023],\n",
       "        [ 0.0200,  0.0100, -0.0208,  ...,  0.0234,  0.0090, -0.0013],\n",
       "        [-0.0151, -0.0035,  0.0219,  ..., -0.0092, -0.0283,  0.0043],\n",
       "        ...,\n",
       "        [-0.0415, -0.0522, -0.0233,  ...,  0.0427, -0.0374,  0.0063],\n",
       "        [-0.0398, -0.0284, -0.0083,  ...,  0.0378, -0.0408, -0.0204],\n",
       "        [ 0.0491, -0.0129,  0.0015,  ..., -0.0439,  0.0197,  0.0352]],\n",
       "       device='cuda:4', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['model.layers.0.self_attn.q_proj.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17778ed-8eab-43f7-b9d6-22c4862d46c8",
   "metadata": {},
   "source": [
    "**微调的目的**：对这些参数中的全部或部分进行调整，以便它能更适配目标场景的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa895190-f010-41ce-a07a-d6b8c4538659",
   "metadata": {},
   "source": [
    "**小结**：本文以使用模型作为起点，由前台到后台，由外及里，逐层解开模型生成文本的整个过程。目的是让学习者快速理解生成式语言模型的基本工作原理，在这个过程中，我们详细讨论了文本生成的细节，并提供了可操作的代码块，以方便感兴趣的同学自己动手尝试。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
