{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2181f2e-8394-4763-ae01-dff717010041",
   "metadata": {},
   "source": [
    "## 前言\n",
    "\n",
    "LLama.cpp是由Georgi Gerganov开发的一个开源工具，主要用于将大语言模型（LLM）转换为C++代码，使它们可以在任意的CPU设备上运行。\n",
    "\n",
    "它的优势在于：\n",
    "- 无需依赖pytorch和python，而是以c++编译的可执行文件来运行。\n",
    "- 支持丰富的硬件设备，包括Nvidia、AMD、Intel、Apple Silicon、华为昇腾等芯片。\n",
    "- 支持f16和f32混合精度，也支持8位、4位甚至1位的量化来加快推理。\n",
    "- 无需GPU，可只用CPU运行，甚至可以在Android设备上运行。\n",
    "\n",
    "本文我们将用llama.cpp来运行之前微调过的欺诈文本分类模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d457dbe-1880-4d48-a60e-2e4ba493e5c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d50a29d-9d38-4f65-b5ac-bcdd315a7467",
   "metadata": {},
   "source": [
    "使用llama-server运行gguf, 并通过`http://192.168.31.200:8080/`来访问 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8d7ef-ee5b-4b8e-9b6d-edb15d7c67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/data2/downloads/llama.cpp/llama-server -m /data2/anti_fraud/models/Qwen2-1___5B-anti_fraud_1__1/model-BF16.gguf -ngl 28 -fa --host 0.0.0.0 --port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643504e-10c3-4790-8258-599a2fae376f",
   "metadata": {},
   "source": [
    "## 模型文件转换\n",
    "我们微调后的模型由两部分组成：基座模型和Lora适配器，需要对这两者分别转换，最后再合并。\n",
    "\n",
    "先用`convert_hf_to_gguf.py`工具转换基座模型。\n",
    "\n",
    "> 注：convert_hf_to_gguf.py是llama.cpp提供的工具脚本，位于安装目录下，用于将huggingface上下载的safetensors模型格式转换为gguf文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79dd7f49-7f34-46b9-aa0b-beb7dbdf70f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: Qwen2-1___5B-Instruct\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {1536, 151936}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 1536\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8960\n",
      "INFO:hf-to-gguf:gguf: head count = 12\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 2\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151643\n",
      "INFO:gguf.vocab:Setting special token type bos to 151643\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/data2/anti_fraud/models/anti_fraud_v11/qwen2_bf16.gguf: n_tensors = 338, total_size = 3.1G\n",
      "Writing: 100%|██████████████████████████| 3.09G/3.09G [00:39<00:00, 77.9Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /data2/anti_fraud/models/anti_fraud_v11/qwen2_bf16.gguf\n"
     ]
    }
   ],
   "source": [
    "!python /data2/downloads/llama.cpp/convert_hf_to_gguf.py \\\n",
    "    --outtype bf16 \\\n",
    "    --outfile /data2/anti_fraud/models/anti_fraud_v11/qwen2_bf16.gguf \\\n",
    "    --model-name qwen2 \\\n",
    "    /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198e658-f3c0-41a0-94b0-768c31d59e0d",
   "metadata": {},
   "source": [
    "接下来使用`convert_lora_to_gguf.py `脚本工具来转换lora适配器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2b9142-5206-4c21-954b-c4e491de41c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:lora-to-gguf:Loading base model: Qwen2-1___5B-Instruct\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:lora-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight.lora_a, torch.float32 --> BF16, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight.lora_b, torch.float32 --> BF16, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight.lora_b, torch.float32 --> BF16, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight.lora_a, torch.float32 --> BF16, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight.lora_b, torch.float32 --> BF16, shape = {16, 256}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 1536\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8960\n",
      "INFO:hf-to-gguf:gguf: head count = 12\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 2\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "DEBUG:hf-to-gguf:chktok: [198, 4710, 14731, 65497, 7847, 1572, 2303, 78672, 10947, 145836, 320, 8252, 8, 26525, 114, 378, 235, 149921, 30543, 320, 35673, 99066, 97534, 8, 25521, 227, 11162, 99, 247, 149955, 220, 18, 220, 18, 18, 220, 18, 18, 18, 220, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 18, 18, 220, 18, 18, 18, 18, 18, 18, 18, 18, 220, 18, 13, 18, 220, 18, 496, 18, 220, 18, 1112, 18, 220, 146394, 97529, 241, 44258, 233, 146568, 44258, 224, 147603, 20879, 115, 146280, 44258, 223, 146280, 147272, 97529, 227, 144534, 937, 104100, 18493, 22377, 99257, 16, 18, 16, 19, 16, 20, 16, 35727, 21216, 55460, 53237, 18658, 14144, 1456, 13073, 63471, 33594, 3038, 133178, 79012, 3355, 4605, 4605, 13874, 13874, 73594, 3014, 3014, 28149, 17085, 2928, 26610, 7646, 358, 3003, 1012, 364, 83, 813, 566, 594, 1052, 11, 364, 787, 498, 2704, 30, 364, 44, 537, 2704, 358, 3278, 1281, 432, 11, 364, 35, 498, 1075, 1045, 15243, 30, 1205, 6, 42612, 264, 63866, 43]\n",
      "DEBUG:hf-to-gguf:chkhsh: e636dc30a262dcc0d8c323492e32ae2b70728f4df7dfe9737d9f920a282b8aea\n",
      "DEBUG:hf-to-gguf:tokenizer.ggml.pre: 'qwen2'\n",
      "DEBUG:hf-to-gguf:chkhsh: e636dc30a262dcc0d8c323492e32ae2b70728f4df7dfe9737d9f920a282b8aea\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151643\n",
      "INFO:gguf.vocab:Setting special token type bos to 151643\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/data2/anti_fraud/models/anti_fraud_v11/lora_0913_4_bf16.gguf: n_tensors = 392, total_size = 36.9M\n",
      "Writing: 100%|██████████████████████████| 36.9M/36.9M [00:01<00:00, 21.4Mbyte/s]\n",
      "INFO:lora-to-gguf:Model successfully exported to /data2/anti_fraud/models/anti_fraud_v11/lora_0913_4_bf16.gguf\n"
     ]
    }
   ],
   "source": [
    "!python /data2/downloads/llama.cpp/convert_lora_to_gguf.py \\\n",
    "    --base /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct \\\n",
    "    --outfile /data2/anti_fraud/models/anti_fraud_v11/lora_0913_4_bf16.gguf \\\n",
    "    /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0913_4/checkpoint-5454 \\\n",
    "    --outtype bf16 --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0f349-4a82-46e3-869c-b7b83e9e8eab",
   "metadata": {},
   "source": [
    "执行完后，得到一个Lora适配器的gguf文件`lora_0913_4_bf16.gguf`。\n",
    "\n",
    "使用`llama-export-lora`工具将基座模型和Lora适配器合并为一个gguf文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e4b82c3-8c95-482f-8fae-96c1ec767a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_input: loaded gguf from /data2/anti_fraud/models/anti_fraud_v11/qwen2_bf16.gguf\n",
      "file_input: loaded gguf from /data2/anti_fraud/models/anti_fraud_v11/lora_0913_4_bf16.gguf\n",
      "copy_tensor :  blk.0.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.0.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.0.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.0.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.0.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.0.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.0.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.0.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.0.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.0.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.0.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.0.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.1.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.1.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.1.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.1.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.1.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.1.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.1.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.1.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.1.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.1.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.1.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.1.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.10.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.10.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.10.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.10.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.10.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.10.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.10.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.10.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.10.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.10.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.10.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.10.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.11.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.11.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.11.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.11.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.11.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.11.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.11.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.11.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.11.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.11.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.11.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.11.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.12.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.12.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.12.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.12.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.12.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.12.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.12.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.12.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.12.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.12.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.12.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.12.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.13.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.13.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.13.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.13.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.13.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.13.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.13.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.13.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.13.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.13.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.13.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.13.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.14.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.14.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.14.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.14.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.14.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.14.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.14.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.14.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.14.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.14.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.14.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.14.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.15.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.15.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.15.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.15.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.15.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.15.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.15.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.15.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.15.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.15.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.15.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.15.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.16.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.16.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.16.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.16.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.16.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.16.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.16.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.16.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.16.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.16.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.16.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.16.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.17.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.17.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.17.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.17.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.17.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.17.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.17.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.17.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.17.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.17.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.17.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.17.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.18.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.18.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.18.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.18.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.18.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.18.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.18.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.18.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.18.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.18.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.18.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.18.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.19.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.19.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.19.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.19.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.19.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.19.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.19.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.19.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.19.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.19.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.19.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.19.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.2.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.2.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.2.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.2.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.2.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.2.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.2.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.2.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.2.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.2.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.2.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.2.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.20.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.20.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.20.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.20.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.20.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.20.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.20.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.20.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.20.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.20.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.20.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.20.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.21.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.21.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.21.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.21.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.21.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.21.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.21.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.21.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.21.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.21.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.21.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.21.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.22.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.22.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.22.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.22.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.22.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.22.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.22.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.22.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.22.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.22.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.22.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.22.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.23.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.23.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.23.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.23.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.23.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.23.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.23.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.23.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.23.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.23.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.23.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.23.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.24.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.24.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.24.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.24.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.24.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.24.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.24.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.24.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.24.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.24.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.24.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.24.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.25.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.25.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.25.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.25.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.25.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.25.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.25.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.25.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.25.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.25.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.25.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.25.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.26.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.26.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.26.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.26.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.26.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.26.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.26.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.26.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.26.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.26.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.26.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.26.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.27.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.27.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.27.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.27.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.27.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.27.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.27.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.27.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.27.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.27.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.27.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.27.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.3.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.3.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.3.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.3.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.3.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.3.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.3.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.3.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.3.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.3.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.3.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.3.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.4.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.4.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.4.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.4.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.4.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.4.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.4.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.4.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.4.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.4.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.4.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.4.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.5.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.5.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.5.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.5.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.5.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.5.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.5.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.5.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.5.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.5.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.5.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.5.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.6.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.6.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.6.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.6.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.6.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.6.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.6.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.6.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.6.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.6.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.6.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.6.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.7.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.7.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.7.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.7.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.7.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.7.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.7.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.7.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.7.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.7.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.7.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.7.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.8.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.8.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.8.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.8.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.8.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.8.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.8.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.8.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.8.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.8.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.8.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.8.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.9.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.9.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.9.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.9.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.9.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.9.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.9.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.9.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.9.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.9.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.9.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.9.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=bf16\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  output_norm.weight [1536, 1, 1, 1]\n",
      "copy_tensor :  token_embd.weight [1536, 151936, 1, 1]\n",
      "run_merge : merged 196 tensors with lora adapters\n",
      "run_merge : wrote 338 tensors to output file\n",
      "done, output file is /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf\n"
     ]
    }
   ],
   "source": [
    "!/data2/downloads/llama.cpp/llama-export-lora \\\n",
    "    -m /data2/anti_fraud/models/anti_fraud_v11/qwen2_bf16.gguf \\\n",
    "    -o /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf \\\n",
    "    --lora /data2/anti_fraud/models/anti_fraud_v11/lora_0913_4_bf16.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a39e4f-114d-43e5-96ff-aa36e96bd02b",
   "metadata": {},
   "source": [
    "\n",
    "查看导出的文件：\n",
    "\n",
    "```python\n",
    "-rw-rw-r-- 1   42885408 Nov  9 14:57 lora_0913_4_bf16.gguf\n",
    "-rw-rw-r-- 1 3093666720 Nov  9 14:58 model_bf16.gguf\n",
    "-rw-rw-r-- 1 3093666720 Nov  9 14:56 qwen2_bf16.gguf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe4981-736d-453a-a3c7-20a78dcde858",
   "metadata": {},
   "source": [
    "经过上面三步，我们就将safetensors格式的基座模型和lora适配器导出为gguf格式的模型文件`model_bf16.gguf`，此时模型文件大小并没有变化，仍然有3G。\n",
    "\n",
    "用`llama-cli`命令验证此模型文件是否能正常work。\n",
    "> llama-cli是一种命令行接口，允许用户只通过一条命令完成模型启动和模型访问，用于快速测试和调试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db20090-9c0d-4652-ab5f-982788ad2791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个来自太行山下小村庄家的孩子，从小生活在乡下，对乡下充满了深深的思念，也对乡下人有着深深的敬仰。从小我便梦想着能够走出大山，去看看外面的世界，见识不一样的风景。如今，这个梦想终于成真，我如愿以偿地来到了这个繁华的城市，开始了全新的生活。\n",
      "初到城市的时候，我感到既兴奋又迷茫，兴奋的是能够接触到不同的文化，接触到外面的世界；迷茫的是，自己曾经的生活环境和习惯突然改变，需要"
     ]
    }
   ],
   "source": [
    "!/data2/downloads/llama.cpp/llama-cli --log-disable \\\n",
    "\t-m /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf \\\n",
    "\t-p \"我是一个来自太行山下小村庄家的孩子\" \\\n",
    "\t-n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a94d55-1cc5-44d8-a9be-f9ac802061ea",
   "metadata": {},
   "source": [
    "## 量化\n",
    "\n",
    "使用`llama-quantize`工具将模型文件由16位量化为8位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48d79498-b525-4547-9e17-745deb3b3837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 3646 (cddae488)\n",
      "main: built with cc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0 for x86_64-linux-gnu\n",
      "main: quantizing '/data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf' to '/data2/anti_fraud/models/anti_fraud_v11/model_bf16_q8_0.gguf' as Q8_0\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 338 tensors from /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = qwen2\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = 1___5B-Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1.5B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  10:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     qwen2.embedding_length u32              = 1536\n",
      "llama_model_loader: - kv  12:                  qwen2.feed_forward_length u32              = 8960\n",
      "llama_model_loader: - kv  13:                 qwen2.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv  14:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  15:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  16:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type  f16:  196 tensors\n",
      "llama_model_loader: - type bf16:    1 tensors\n",
      "[   1/ 338]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[   2/ 338]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[   3/ 338]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[   4/ 338]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[   5/ 338]                    blk.0.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[   6/ 338]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[   7/ 338]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[   8/ 338]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[   9/ 338]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  10/ 338]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  11/ 338]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  12/ 338]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  13/ 338]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  14/ 338]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  15/ 338]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  16/ 338]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  17/ 338]                    blk.1.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  18/ 338]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  19/ 338]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  20/ 338]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  21/ 338]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  22/ 338]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  23/ 338]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  24/ 338]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  25/ 338]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  26/ 338]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  27/ 338]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  28/ 338]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  29/ 338]                   blk.10.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  30/ 338]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  31/ 338]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  32/ 338]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  33/ 338]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  34/ 338]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  35/ 338]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  36/ 338]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  37/ 338]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  38/ 338]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  39/ 338]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  40/ 338]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  41/ 338]                   blk.11.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  42/ 338]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  43/ 338]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  44/ 338]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  45/ 338]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  46/ 338]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  47/ 338]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  48/ 338]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  49/ 338]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  50/ 338]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  51/ 338]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  52/ 338]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  53/ 338]                   blk.12.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  54/ 338]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  55/ 338]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  56/ 338]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  57/ 338]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  58/ 338]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  59/ 338]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  60/ 338]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  61/ 338]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  62/ 338]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  63/ 338]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  64/ 338]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  65/ 338]                   blk.13.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  66/ 338]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  67/ 338]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  68/ 338]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  69/ 338]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  70/ 338]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  71/ 338]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  72/ 338]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  73/ 338]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  74/ 338]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  75/ 338]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  76/ 338]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  77/ 338]                   blk.14.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  78/ 338]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  79/ 338]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  80/ 338]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  81/ 338]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  82/ 338]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  83/ 338]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  84/ 338]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  85/ 338]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  86/ 338]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  87/ 338]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  88/ 338]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  89/ 338]                   blk.15.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  90/ 338]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  91/ 338]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  92/ 338]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  93/ 338]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  94/ 338]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  95/ 338]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  96/ 338]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  97/ 338]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  98/ 338]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  99/ 338]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 100/ 338]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 101/ 338]                   blk.16.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 102/ 338]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 103/ 338]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 104/ 338]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 105/ 338]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 106/ 338]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 107/ 338]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 108/ 338]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 109/ 338]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 110/ 338]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 111/ 338]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 112/ 338]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 113/ 338]                   blk.17.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 114/ 338]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 115/ 338]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 116/ 338]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 117/ 338]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 118/ 338]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 119/ 338]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 120/ 338]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 121/ 338]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 122/ 338]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 123/ 338]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 124/ 338]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 125/ 338]                   blk.18.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 126/ 338]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 127/ 338]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 128/ 338]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 129/ 338]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 130/ 338]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 131/ 338]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 132/ 338]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 133/ 338]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 134/ 338]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 135/ 338]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 136/ 338]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 137/ 338]                   blk.19.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 138/ 338]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 139/ 338]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 140/ 338]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 141/ 338]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 142/ 338]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 143/ 338]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 144/ 338]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 145/ 338]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 146/ 338]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 147/ 338]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 148/ 338]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 149/ 338]                    blk.2.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 150/ 338]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 151/ 338]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 152/ 338]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 153/ 338]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 154/ 338]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 155/ 338]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 156/ 338]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 157/ 338]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 158/ 338]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 159/ 338]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 160/ 338]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 161/ 338]                   blk.20.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 162/ 338]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 163/ 338]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 164/ 338]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 165/ 338]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 166/ 338]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 167/ 338]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 168/ 338]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 169/ 338]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 170/ 338]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 171/ 338]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 172/ 338]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 173/ 338]                   blk.21.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 174/ 338]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 175/ 338]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 176/ 338]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 177/ 338]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 178/ 338]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 179/ 338]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 180/ 338]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 181/ 338]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 182/ 338]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 183/ 338]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 184/ 338]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 185/ 338]                   blk.22.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 186/ 338]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 187/ 338]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 188/ 338]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 189/ 338]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 190/ 338]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 191/ 338]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 192/ 338]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 193/ 338]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 194/ 338]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 195/ 338]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 196/ 338]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 197/ 338]                   blk.23.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 198/ 338]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 199/ 338]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 200/ 338]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 201/ 338]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 202/ 338]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 203/ 338]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 204/ 338]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 205/ 338]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 206/ 338]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 207/ 338]              blk.24.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 208/ 338]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 209/ 338]                   blk.24.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 210/ 338]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 211/ 338]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 212/ 338]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 213/ 338]               blk.24.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 214/ 338]               blk.24.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 215/ 338]               blk.24.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 216/ 338]                 blk.24.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 217/ 338]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 218/ 338]                 blk.25.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 219/ 338]              blk.25.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 220/ 338]            blk.25.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 221/ 338]                   blk.25.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 222/ 338]                 blk.25.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 223/ 338]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 224/ 338]                 blk.25.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 225/ 338]               blk.25.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 226/ 338]               blk.25.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 227/ 338]               blk.25.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 228/ 338]                 blk.25.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 229/ 338]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 230/ 338]                 blk.26.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 231/ 338]              blk.26.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 232/ 338]            blk.26.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 233/ 338]                   blk.26.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 234/ 338]                 blk.26.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 235/ 338]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 236/ 338]                 blk.26.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 237/ 338]               blk.26.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 238/ 338]               blk.26.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 239/ 338]               blk.26.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 240/ 338]                 blk.26.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 241/ 338]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 242/ 338]                 blk.27.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 243/ 338]              blk.27.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 244/ 338]            blk.27.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 245/ 338]                   blk.27.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 246/ 338]                 blk.27.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 247/ 338]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 248/ 338]                 blk.27.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 249/ 338]               blk.27.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 250/ 338]               blk.27.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 251/ 338]               blk.27.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 252/ 338]                 blk.27.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 253/ 338]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 254/ 338]                  blk.3.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 255/ 338]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 256/ 338]             blk.3.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 257/ 338]                    blk.3.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 258/ 338]                  blk.3.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 259/ 338]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 260/ 338]                  blk.3.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 261/ 338]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 262/ 338]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 263/ 338]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 264/ 338]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 265/ 338]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 266/ 338]                  blk.4.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 267/ 338]               blk.4.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 268/ 338]             blk.4.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 269/ 338]                    blk.4.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 270/ 338]                  blk.4.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 271/ 338]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 272/ 338]                  blk.4.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 273/ 338]                blk.4.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 274/ 338]                blk.4.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 275/ 338]                blk.4.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 276/ 338]                  blk.4.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 277/ 338]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 278/ 338]                  blk.5.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 279/ 338]               blk.5.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 280/ 338]             blk.5.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 281/ 338]                    blk.5.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 282/ 338]                  blk.5.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 283/ 338]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 284/ 338]                  blk.5.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 285/ 338]                blk.5.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 286/ 338]                blk.5.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 287/ 338]                blk.5.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 288/ 338]                  blk.5.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 289/ 338]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 290/ 338]                  blk.6.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 291/ 338]               blk.6.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 292/ 338]             blk.6.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 293/ 338]                    blk.6.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 294/ 338]                  blk.6.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 295/ 338]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 296/ 338]                  blk.6.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 297/ 338]                blk.6.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 298/ 338]                blk.6.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 299/ 338]                blk.6.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 300/ 338]                  blk.6.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 301/ 338]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 302/ 338]                  blk.7.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 303/ 338]               blk.7.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 304/ 338]             blk.7.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 305/ 338]                    blk.7.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 306/ 338]                  blk.7.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 307/ 338]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 308/ 338]                  blk.7.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 309/ 338]                blk.7.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 310/ 338]                blk.7.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 311/ 338]                blk.7.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 312/ 338]                  blk.7.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 313/ 338]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 314/ 338]                  blk.8.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 315/ 338]               blk.8.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 316/ 338]             blk.8.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 317/ 338]                    blk.8.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 318/ 338]                  blk.8.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 319/ 338]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 320/ 338]                  blk.8.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 321/ 338]                blk.8.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 322/ 338]                blk.8.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 323/ 338]                blk.8.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 324/ 338]                  blk.8.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 325/ 338]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 326/ 338]                  blk.9.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 327/ 338]               blk.9.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 328/ 338]             blk.9.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 329/ 338]                    blk.9.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 330/ 338]                  blk.9.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 331/ 338]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 332/ 338]                  blk.9.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 333/ 338]                blk.9.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 334/ 338]                blk.9.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 335/ 338]                blk.9.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 336/ 338]                  blk.9.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 337/ 338]                   output_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 338/ 338]                    token_embd.weight - [ 1536, 151936,     1,     1], type =   bf16, converting to q8_0 .. size =   445.12 MiB ->   236.47 MiB\n",
      "llama_model_quantize_internal: model size  =  2944.68 MB\n",
      "llama_model_quantize_internal: quant size  =  1564.62 MB\n",
      "\n",
      "main: quantize time =  4671.71 ms\n",
      "main:    total time =  4671.71 ms\n"
     ]
    }
   ],
   "source": [
    "!/data2/downloads/llama.cpp/llama-quantize \\\n",
    "/data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf /data2/anti_fraud/models/anti_fraud_v11/model_bf16_q8_0.gguf q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f379cf2-b2f0-42f7-9942-5c8ac79a2310",
   "metadata": {},
   "source": [
    "经过量化后，模型文件由`2944.68MB`减小到`1564.62MB`，几乎缩小了一倍。\n",
    "\n",
    "## 转换为ollama文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc9cb32-342c-4557-825a-a3463e907ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM ./model_bf16_q8_0.gguf\n",
      "\n",
      "TEMPLATE \"\"\"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\"\"\"\n",
      "\n",
      "PARAMETER stop \"<|im_end|>\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat /data2/anti_fraud/models/anti_fraud_v11/anti_fraud.modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ae9ad-e901-4b33-9c84-4e19bade606e",
   "metadata": {},
   "source": [
    "> 注：ollama不支持bf16，相关报错信息：Error: invalid file magic\n",
    "[不支持bf16的说明](https://github.com/ollama/ollama/issues/4670)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d833fea5-e650-4cc1-a3a6-39934fd99137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25ltransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer \n",
      "creating template layer \n",
      "creating parameters layer \n",
      "creating config layer ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gtransferring model data \n",
      "creating model layer \n",
      "creating template layer \n",
      "creating parameters layer \n",
      "creating config layer \n",
      "using already created layer sha256:d45d821a725da44cf16e8886cc549b0494b947d53a6240dddd2a82a6dfb29628 \n",
      "writing layer sha256:825696bf1e08913602466db54661a4372508114d8e3e6b9d128fe254625d784c \n",
      "using already created layer sha256:f02dd72bb2423204352eabc5637b44d79d17f109fdb510a7c51455892aa2d216 \n",
      "writing layer sha256:11b469b3a050022ae412c9fa7397321876175009930b02ca1ba36a6a48a6fb76 \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama create qwen2:1.5b -f /data2/anti_fraud/models/anti_fraud_v11/qwen2.modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c64650-5e5c-4b6c-a48c-791007da479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "删除模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f41755-bb2f-40f1-b571-7698cc9cd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama rm qwen2:1.5b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
