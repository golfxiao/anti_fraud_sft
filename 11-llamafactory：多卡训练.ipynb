{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b0fec0-d7ea-47d2-a425-a6d2c69bbc61",
   "metadata": {},
   "source": [
    "## 引言\n",
    "[前文](https://golfxiao.blog.csdn.net/article/details/141440847)训练时都做了一定的编码工作，其实有一些框架可以支持我们零代码微调，[LLama-Factory](https://llamafactory.readthedocs.io/zh-cn/latest/)就是其中一个。这是一个专门针对大语言模型的微调和训练平台，有如下特性：\n",
    "- 支持常见的模型种类：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。 \n",
    "- 支持单GPU和多GPU训练。\n",
    "- 支持全参微调、Lora微调、QLora微调。\n",
    "……\n",
    "还有很多优秀的特性，详细参考：[https://llamafactory.readthedocs.io/zh-cn/latest/](https://llamafactory.readthedocs.io/zh-cn/latest/)\n",
    "\n",
    "本文会尝试用LLamaFactory进行一次多GPU训练。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf290e-9dcf-4782-8580-e9738a47f829",
   "metadata": {},
   "source": [
    "## 数据集准备\n",
    "\n",
    "针对sft， llamafactory支持多种数据格式，我们这里选用alpaca，此格式简单清晰，每条数据只需包含三个字段：\n",
    "- instruction 列对应的内容为人类指令； \n",
    "- input 列对应的内容为人类输入；  \n",
    "- output 列对应的内容为模型回答。\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"计算这些物品的总费用。 \",\n",
    "  \"input\": \"输入：汽车 - $3000，衣服 - $100，书 - $20。\",\n",
    "  \"output\": \"汽车、衣服和书的总费用为 $3000 + $100 + $20 = $3120。\"\n",
    "}\n",
    "```\n",
    "为了格式匹配，封装一个函数`to_alpaca`用于转换数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7fb88-8b6b-4710-9769-53c7fbc9e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def to_alpaca(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:  \n",
    "        dataset = []\n",
    "        for line in infile:  \n",
    "            # 解析每一行的 JSON  \n",
    "            data = json.loads(line)  \n",
    "            response = {\"is_fraud\":data[\"label\"], \"fraud_speaker\":data[\"fraud_speaker\"], \"reason\":data[\"reason\"]}\n",
    "            item = {\n",
    "                'input': data['input'],\n",
    "                'output': json.dumps(response, ensure_ascii=False),\n",
    "                'instruction':data['instruction'],\n",
    "            }  \n",
    "            dataset.append(item)\n",
    "        # 将结果写入输出文件  \n",
    "        outfile.write(json.dumps(dataset, indent=4, ensure_ascii=False))  \n",
    "        print(f\"convert over，{input_path} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f7043-5878-47d8-9913-909a3e7a4173",
   "metadata": {},
   "source": [
    "批量将前一节构建好的数据作格式转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c789e42-1c7d-4337-b715-bb900dbd0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设输入数据存储在 input.jsonl 文件中  \n",
    "input_files = [\n",
    "    '../dataset/fraud/train_test/train0819.jsonl',\n",
    "    '../dataset/fraud/train_test/test0819.jsonl',\n",
    "    '../dataset/fraud/train_test/eval0819.jsonl',\n",
    "]\n",
    "\n",
    "def filename(path):\n",
    "    filename_with_ext = os.path.basename(path)\n",
    "    filename, extention = os.path.splitext(filename_with_ext)\n",
    "    return filename\n",
    "\n",
    "for input_path in input_files:\n",
    "    output_path = f'../dataset/fraud/train_test/{filename(input_path)}_alpaca.jsonl'\n",
    "    to_alpaca(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58dec9-2c85-48e9-b2b1-2eaf285676e8",
   "metadata": {},
   "source": [
    "convert over，../dataset/fraud/train_test/train0819.jsonl to ../dataset/fraud/train_test/train0819_alpaca.json\n",
    "convert over，../dataset/fraud/train_test/test0819.jsonl to ../dataset/fraud/train_test/test0819_alpaca.json\n",
    "convert over，../dataset/fraud/train_test/eval0819.jsonl to ../dataset/fraud/train_test/eval0819_alpaca.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b1133-3d70-441e-8b73-cb26d0874e72",
   "metadata": {},
   "source": [
    "转换好数据集后，需要将其配置到LLamaFactory安装目录下的`data/dataset_info.json`文件中，只需要在文件最后添加我们新构造的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6486af-3447-4297-983c-c93aff8c3acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"identity\": {\n",
    "    \"file_name\": \"identity.json\"\n",
    "  },\n",
    "  ……\n",
    "  \"anti_fraud\": {\n",
    "    \"file_name\": \"train0819_alpaca.jsonl\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9850eb9-743f-4a46-bd0b-19d877c3ae4a",
   "metadata": {},
   "source": [
    "## 参数配置\n",
    "LLamaFactory的训练参数采用yaml文件保存，在安装目录下的`examples`子目录下有各种微调方法的示例配置，可以直接拷贝一份进行修改。\n",
    "\n",
    "![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/95c191a523e840fc969c0d014c82047e.png)\n",
    "\n",
    "查看配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28bfe549-1b58-49e1-a36a-b67b4e251c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### model\n",
      "model_name_or_path: /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct\n",
      "resume_from_checkpoint: /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1200\n",
      "\n",
      "### method\n",
      "stage: sft\n",
      "do_train: true\n",
      "finetuning_type: lora\n",
      "lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\n",
      "lora_rank: 16\n",
      "lora_alpha: 32\n",
      "lora_dropout: 0.2\n",
      "\n",
      "\n",
      "### dataset\n",
      "dataset_dir: /data2/downloads/LLaMA-Factory/data\n",
      "dataset: anti_fraud\n",
      "template: qwen\n",
      "cutoff_len: 1024\n",
      "max_samples: 200000\n",
      "overwrite_cache: true\n",
      "preprocessing_num_workers: 16\n",
      "\n",
      "### output\n",
      "output_dir: /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826\n",
      "logging_steps: 10\n",
      "save_steps: 100\n",
      "plot_loss: true\n",
      "overwrite_output_dir: true\n",
      "\n",
      "### train\n",
      "per_device_train_batch_size: 16\n",
      "gradient_accumulation_steps: 1\n",
      "gradient_checkpointing: true\n",
      "learning_rate: 1.0e-4\n",
      "num_train_epochs: 10.0\n",
      "lr_scheduler_type: cosine\n",
      "warmup_ratio: 0.05\n",
      "bf16: true\n",
      "ddp_timeout: 180000000\n",
      "\n",
      "### eval\n",
      "val_size: 0.1\n",
      "per_device_eval_batch_size: 8\n",
      "eval_strategy: steps\n",
      "eval_steps: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat /data2/downloads/LLaMA-Factory/qwen2_lora_sft.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9535ebc-0d33-41c8-8edc-2dc26313c2f1",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "设置环境变量CUDA_VISIBLE_DEVICES声明训练过程中允许使用4张显卡，显卡编号分别为1、2、3、4。\n",
    "\n",
    "使用\t`llamafactory-cli`命令启动训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b22ae5-c4dd-44c8-9130-f0bd87032421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b127ae5-742d-43f6-ab61-2160635cf233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-27 18:06:56,229] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "WARNING 08-27 18:06:58 _custom_ops.py:14] Failed to import from vllm._C with ImportError('/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN5torch3jit11parseSchemaERKSs')\n",
      "08/27/2024 18:07:00 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:20962\n",
      "W0827 18:07:02.274000 139932174230464 torch/distributed/run.py:779] \n",
      "W0827 18:07:02.274000 139932174230464 torch/distributed/run.py:779] *****************************************\n",
      "W0827 18:07:02.274000 139932174230464 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0827 18:07:02.274000 139932174230464 torch/distributed/run.py:779] *****************************************\n",
      "[2024-08-27 18:07:06,832] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-08-27 18:07:06,891] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-08-27 18:07:06,977] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "[2024-08-27 18:07:06,991] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "WARNING 08-27 18:07:08 _custom_ops.py:14] Failed to import from vllm._C with ImportError('/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN5torch3jit11parseSchemaERKSs')\n",
      "WARNING 08-27 18:07:08 _custom_ops.py:14] Failed to import from vllm._C with ImportError('/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN5torch3jit11parseSchemaERKSs')\n",
      "WARNING 08-27 18:07:08 _custom_ops.py:14] Failed to import from vllm._C with ImportError('/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN5torch3jit11parseSchemaERKSs')\n",
      "WARNING 08-27 18:07:08 _custom_ops.py:14] Failed to import from vllm._C with ImportError('/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/vllm/_C.abi3.so: undefined symbol: _ZN5torch3jit11parseSchemaERKSs')\n",
      "08/27/2024 18:07:11 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "08/27/2024 18:07:11 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "08/27/2024 18:07:11 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "08/27/2024 18:07:11 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "08/27/2024 18:07:11 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "08/27/2024 18:07:11 - INFO - llamafactory.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "08/27/2024 18:07:11 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "08/27/2024 18:07:11 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-27 18:07:11,780 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-27 18:07:11,780 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-27 18:07:11,780 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-27 18:07:11,780 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-27 18:07:11,780 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-27 18:07:11,780 >> loading file tokenizer_config.json\n",
      "08/27/2024 18:07:11 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "08/27/2024 18:07:11 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "08/27/2024 18:07:11 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "[INFO|tokenization_utils_base.py:2533] 2024-08-27 18:07:12,037 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "08/27/2024 18:07:12 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "08/27/2024 18:07:12 - INFO - llamafactory.data.loader - Loading dataset train0819_alpaca.jsonl...\n",
      "Converting format of dataset (num_proc=16): 100%|█| 21135/21135 [00:00<00:00, 69\n",
      "08/27/2024 18:07:13 - INFO - llamafactory.data.loader - Loading dataset train0819_alpaca.jsonl...\n",
      "08/27/2024 18:07:13 - INFO - llamafactory.data.loader - Loading dataset train0819_alpaca.jsonl...\n",
      "08/27/2024 18:07:13 - INFO - llamafactory.data.loader - Loading dataset train0819_alpaca.jsonl...\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 21135/21135 [00:03<00:00, 64\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 271, 100431, 99639, 37474, 105051, 108704, 11, 220, 14880, 101042, 105051, 43815, 107189, 106037, 101052, 3837, 23031, 2236, 68805, 66017, 103929, 104317, 59151, 9623, 761, 97957, 25, 830, 91233, 8, 3407, 110395, 18, 25, 10236, 236, 108, 102865, 101393, 99487, 101314, 100006, 101189, 100006, 85336, 99360, 102683, 99225, 106630, 104528, 3837, 85336, 26939, 99487, 104671, 100634, 20412, 104917, 100634, 99557, 104366, 115203, 99487, 108398, 100634, 99650, 104468, 3837, 99650, 99725, 100662, 99792, 99692, 46944, 46944, 104160, 32757, 8997, 110395, 16, 25, 58230, 109, 20412, 151645, 198, 151644, 77091, 198, 4913, 285, 761, 97957, 788, 895, 92, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "下面是一段对话文本, 请分析对话内容是否有诈骗风险，以json格式输出你的判断结果(is_fraud: true/false)。\n",
      "\n",
      "发言人3: 现在我所在这个哪里能够工艺能够去把屈光做得很好的，去到这个省级医院是自治区医院跟广西医科大学这个附属医院他们还可以，他们一直保持比较好的一个一个手术量。\n",
      "发言人1: 就是<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"is_fraud\": false}<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4913, 285, 761, 97957, 788, 895, 92, 151645]\n",
      "labels:\n",
      "{\"is_fraud\": false}<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:07:17,722 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:07:17,724 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3631] 2024-08-27 18:07:17,755 >> loading weights file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1572] 2024-08-27 18:07:17,767 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1038] 2024-08-27 18:07:17,771 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.loader - trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "08/27/2024 18:08:51 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "08/27/2024 18:08:52 - INFO - llamafactory.model.loader - trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n",
      "08/27/2024 18:08:52 - INFO - llamafactory.model.loader - trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n",
      "[INFO|modeling_utils.py:4463] 2024-08-27 18:09:12,583 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4471] 2024-08-27 18:09:12,583 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:991] 2024-08-27 18:09:12,586 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-27 18:09:12,586 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "08/27/2024 18:09:12 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "08/27/2024 18:09:12 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "08/27/2024 18:09:12 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "08/27/2024 18:09:12 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "08/27/2024 18:09:13 - INFO - llamafactory.model.loader - trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:648] 2024-08-27 18:09:13,376 >> Using auto half precision backend\n",
      "08/27/2024 18:09:13 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.\n",
      "[INFO|trainer.py:2526] 2024-08-27 18:09:13,377 >> Loading model from /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1200.\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "[INFO|trainer.py:2134] 2024-08-27 18:09:15,114 >> ***** Running training *****\n",
      "[INFO|trainer.py:2135] 2024-08-27 18:09:15,114 >>   Num examples = 19,021\n",
      "[INFO|trainer.py:2136] 2024-08-27 18:09:15,114 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:2137] 2024-08-27 18:09:15,114 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:2140] 2024-08-27 18:09:15,114 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:2141] 2024-08-27 18:09:15,114 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2142] 2024-08-27 18:09:15,114 >>   Total optimization steps = 2,980\n",
      "[INFO|trainer.py:2143] 2024-08-27 18:09:15,120 >>   Number of trainable parameters = 18,464,768\n",
      "[INFO|trainer.py:2165] 2024-08-27 18:09:15,121 >>   Continuing training from checkpoint, will skip to saved global_step\n",
      "[INFO|trainer.py:2166] 2024-08-27 18:09:15,121 >>   Continuing training from epoch 4\n",
      "[INFO|trainer.py:2167] 2024-08-27 18:09:15,121 >>   Continuing training from global step 1200\n",
      "[INFO|trainer.py:2169] 2024-08-27 18:09:15,121 >>   Will skip the first 4 epochs then the first 8 batches in the first epoch.\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py:2833: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "  0%|                                                  | 0/2980 [00:00<?, ?it/s]/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py:2833: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py:2833: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py:2833: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0049, 'grad_norm': 0.5059604644775391, 'learning_rate': 6.916620574933121e-05, 'epoch': 4.06}\n",
      "{'loss': 0.0072, 'grad_norm': 0.06460018455982208, 'learning_rate': 6.865256377874144e-05, 'epoch': 4.09}\n",
      "{'loss': 0.0092, 'grad_norm': 0.2509661316871643, 'learning_rate': 6.81366248447076e-05, 'epoch': 4.13}\n",
      "{'loss': 0.006, 'grad_norm': 0.2362297624349594, 'learning_rate': 6.76184524823493e-05, 'epoch': 4.16}\n",
      "{'loss': 0.0054, 'grad_norm': 0.2604801058769226, 'learning_rate': 6.709811050182088e-05, 'epoch': 4.19}\n",
      "{'loss': 0.004, 'grad_norm': 0.14560535550117493, 'learning_rate': 6.657566298045362e-05, 'epoch': 4.23}\n",
      "{'loss': 0.0064, 'grad_norm': 0.823914110660553, 'learning_rate': 6.605117425486482e-05, 'epoch': 4.26}\n",
      "{'loss': 0.0058, 'grad_norm': 0.21819910407066345, 'learning_rate': 6.55247089130352e-05, 'epoch': 4.3}\n",
      "{'loss': 0.0072, 'grad_norm': 0.139102965593338, 'learning_rate': 6.499633178635516e-05, 'epoch': 4.33}\n",
      "{'loss': 0.0086, 'grad_norm': 0.23780354857444763, 'learning_rate': 6.446610794164122e-05, 'epoch': 4.36}\n",
      " 44%|█████████████████                      | 1300/2980 [03:11<51:01,  1.82s/it][INFO|trainer.py:3819] 2024-08-27 18:12:26,587 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:12:26,587 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:12:26,587 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.43it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.31it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.08it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.60it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:13,  4.36it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.07it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.16it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.16it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.05it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.97it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.04it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.01it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.90it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.03it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.95it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.95it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  4.00it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.03it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.97it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:11,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.98it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.93it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.91it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.97it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.01it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.95it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.05it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.97it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.86it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.84it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.90it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.86it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.91it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.87it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.89it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.82it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.81it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.84it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.99it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.86it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.92it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.88it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.86it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.96it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.99it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.04it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.08it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.95it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.15it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.11it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.61it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.66it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.52it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.55it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.65it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.77it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.94it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.88it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.01812315359711647, 'eval_runtime': 17.1276, 'eval_samples_per_second': 123.427, 'eval_steps_per_second': 3.912, 'epoch': 4.36}\n",
      " 44%|█████████████████                      | 1300/2980 [03:28<51:01,  1.82s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:12:43,718 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1300\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:12:43,750 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:12:43,751 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:12:43,929 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:12:43,929 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1300/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0043, 'grad_norm': 0.2916662395000458, 'learning_rate': 6.393410267312337e-05, 'epoch': 4.4}\n",
      "{'loss': 0.0056, 'grad_norm': 0.1368754357099533, 'learning_rate': 6.34003814944043e-05, 'epoch': 4.43}\n",
      "{'loss': 0.0047, 'grad_norm': 0.4522205591201782, 'learning_rate': 6.286501013039197e-05, 'epoch': 4.46}\n",
      "{'loss': 0.0072, 'grad_norm': 0.23626697063446045, 'learning_rate': 6.232805450920577e-05, 'epoch': 4.5}\n",
      "{'loss': 0.0069, 'grad_norm': 0.1605559140443802, 'learning_rate': 6.178958075405794e-05, 'epoch': 4.53}\n",
      "{'loss': 0.0053, 'grad_norm': 0.19912326335906982, 'learning_rate': 6.124965517511072e-05, 'epoch': 4.56}\n",
      "{'loss': 0.004, 'grad_norm': 0.22060105204582214, 'learning_rate': 6.0708344261310765e-05, 'epoch': 4.6}\n",
      "{'loss': 0.0057, 'grad_norm': 0.1766652911901474, 'learning_rate': 6.016571467220126e-05, 'epoch': 4.63}\n",
      "{'loss': 0.0055, 'grad_norm': 0.3010304868221283, 'learning_rate': 5.9621833229713197e-05, 'epoch': 4.66}\n",
      "{'loss': 0.0067, 'grad_norm': 0.1375969499349594, 'learning_rate': 5.907676690993664e-05, 'epoch': 4.7}\n",
      " 47%|██████████████████▎                    | 1400/2980 [06:39<51:04,  1.94s/it][INFO|trainer.py:3819] 2024-08-27 18:15:55,064 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:15:55,064 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:15:55,064 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.43it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.29it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.08it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.59it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:13,  4.36it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.07it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.16it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.17it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.01it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.94it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.02it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  3.99it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.89it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.93it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.96it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.97it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.90it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.95it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.00it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.94it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.04it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.96it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.85it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.83it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.89it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.86it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.91it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.88it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.83it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.89it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.86it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.95it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.99it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.03it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.07it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.15it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.10it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.61it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.66it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.51it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.55it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.65it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.68it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.75it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.93it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.88it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.013749453239142895, 'eval_runtime': 17.1544, 'eval_samples_per_second': 123.233, 'eval_steps_per_second': 3.906, 'epoch': 4.7}\n",
      " 47%|██████████████████▎                    | 1400/2980 [06:57<51:04,  1.94s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:16:12,220 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1400\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:16:12,261 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:16:12,262 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:16:12,420 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:16:12,420 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1400/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0064, 'grad_norm': 0.0796177089214325, 'learning_rate': 5.8530582834872924e-05, 'epoch': 4.73}\n",
      "{'loss': 0.0043, 'grad_norm': 0.3612557649612427, 'learning_rate': 5.798334826416901e-05, 'epoch': 4.77}\n",
      "{'loss': 0.0072, 'grad_norm': 0.3543163239955902, 'learning_rate': 5.743513058683471e-05, 'epoch': 4.8}\n",
      "{'loss': 0.0035, 'grad_norm': 0.08035508543252945, 'learning_rate': 5.688599731294422e-05, 'epoch': 4.83}\n",
      "{'loss': 0.0097, 'grad_norm': 0.36970555782318115, 'learning_rate': 5.633601606532254e-05, 'epoch': 4.87}\n",
      "{'loss': 0.0093, 'grad_norm': 0.13862678408622742, 'learning_rate': 5.578525457121807e-05, 'epoch': 4.9}\n",
      "{'loss': 0.0048, 'grad_norm': 0.18713702261447906, 'learning_rate': 5.523378065396243e-05, 'epoch': 4.93}\n",
      "{'loss': 0.0056, 'grad_norm': 0.44021734595298767, 'learning_rate': 5.4681662224618316e-05, 'epoch': 4.97}\n",
      "{'loss': 0.0053, 'grad_norm': 0.07136022299528122, 'learning_rate': 5.4128967273616625e-05, 'epoch': 5.0}\n",
      "{'loss': 0.0025, 'grad_norm': 0.06560353934764862, 'learning_rate': 5.357576386238384e-05, 'epoch': 5.03}\n",
      " 50%|███████████████████▋                   | 1500/2980 [10:08<44:54,  1.82s/it][INFO|trainer.py:3819] 2024-08-27 18:19:23,518 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:19:23,518 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:19:23,518 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.40it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.30it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.07it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.59it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.35it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.06it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.15it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.16it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.04it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.96it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.03it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.00it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.89it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.96it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.92it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.98it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.93it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.91it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.97it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.01it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.95it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.04it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.96it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.85it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.83it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.89it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.86it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.92it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.87it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.89it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.82it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.81it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.83it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.88it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.86it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.96it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.99it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.03it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.07it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.15it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.10it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.51it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.54it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.65it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.77it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.95it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.89it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.019067520275712013, 'eval_runtime': 17.1407, 'eval_samples_per_second': 123.332, 'eval_steps_per_second': 3.909, 'epoch': 5.03}\n",
      " 50%|███████████████████▋                   | 1500/2980 [10:25<44:54,  1.82s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:19:40,660 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1500\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:19:40,688 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:19:40,689 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:19:40,877 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:19:40,878 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1500/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0012, 'grad_norm': 0.20183992385864258, 'learning_rate': 5.3022120114960616e-05, 'epoch': 5.07}\n",
      "{'loss': 0.0021, 'grad_norm': 0.002190916333347559, 'learning_rate': 5.246810420961261e-05, 'epoch': 5.1}\n",
      "{'loss': 0.0012, 'grad_norm': 0.003726898692548275, 'learning_rate': 5.191378437043476e-05, 'epoch': 5.13}\n",
      "{'loss': 0.0012, 'grad_norm': 0.08997082710266113, 'learning_rate': 5.135922885894984e-05, 'epoch': 5.17}\n",
      "{'loss': 0.0022, 'grad_norm': 0.26074883341789246, 'learning_rate': 5.0804505965702356e-05, 'epoch': 5.2}\n",
      "{'loss': 0.0027, 'grad_norm': 0.29919296503067017, 'learning_rate': 5.024968400184906e-05, 'epoch': 5.23}\n",
      "{'loss': 0.0011, 'grad_norm': 0.2797108590602875, 'learning_rate': 4.9694831290746657e-05, 'epoch': 5.27}\n",
      "{'loss': 0.0027, 'grad_norm': 0.5178990960121155, 'learning_rate': 4.9140016159538247e-05, 'epoch': 5.3}\n",
      "{'loss': 0.0035, 'grad_norm': 0.3323228061199188, 'learning_rate': 4.858530693073917e-05, 'epoch': 5.34}\n",
      "{'loss': 0.0024, 'grad_norm': 0.29531627893447876, 'learning_rate': 4.803077191382338e-05, 'epoch': 5.37}\n",
      " 54%|████████████████████▉                  | 1600/2980 [13:37<41:01,  1.78s/it][INFO|trainer.py:3819] 2024-08-27 18:22:53,072 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:22:53,073 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:22:53,073 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.40it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.29it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.07it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.59it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.35it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.06it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.15it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.16it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.04it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.97it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.04it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.00it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.89it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.95it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.82it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.84it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.97it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.91it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.96it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.01it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.95it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.04it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.96it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.84it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.79it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.87it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.84it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.90it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.88it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.83it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.88it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.85it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.95it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.03it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.07it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.14it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.09it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.59it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.64it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.50it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.54it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.64it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.94it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.89it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.016932452097535133, 'eval_runtime': 17.1624, 'eval_samples_per_second': 123.177, 'eval_steps_per_second': 3.904, 'epoch': 5.37}\n",
      " 54%|████████████████████▉                  | 1600/2980 [13:55<41:01,  1.78s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:23:10,238 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1600\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:23:10,275 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:23:10,276 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:23:10,463 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:23:10,463 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1600/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0025, 'grad_norm': 0.58893221616745, 'learning_rate': 4.7476479396811674e-05, 'epoch': 5.4}\n",
      "{'loss': 0.0025, 'grad_norm': 0.3367323577404022, 'learning_rate': 4.692249763786224e-05, 'epoch': 5.44}\n",
      "{'loss': 0.0046, 'grad_norm': 0.059070780873298645, 'learning_rate': 4.636889485686503e-05, 'epoch': 5.47}\n",
      "{'loss': 0.002, 'grad_norm': 0.39675670862197876, 'learning_rate': 4.581573922704095e-05, 'epoch': 5.5}\n",
      "{'loss': 0.0017, 'grad_norm': 0.41803452372550964, 'learning_rate': 4.5263098866546586e-05, 'epoch': 5.54}\n",
      "{'loss': 0.0035, 'grad_norm': 0.4146451950073242, 'learning_rate': 4.471104183008592e-05, 'epoch': 5.57}\n",
      "{'loss': 0.0009, 'grad_norm': 0.048535410314798355, 'learning_rate': 4.415963610052962e-05, 'epoch': 5.6}\n",
      "{'loss': 0.0027, 'grad_norm': 0.5515528917312622, 'learning_rate': 4.360894958054345e-05, 'epoch': 5.64}\n",
      "{'loss': 0.0014, 'grad_norm': 0.08418057858943939, 'learning_rate': 4.3059050084226374e-05, 'epoch': 5.67}\n",
      "{'loss': 0.0028, 'grad_norm': 0.8101730942726135, 'learning_rate': 4.2510005328759547e-05, 'epoch': 5.7}\n",
      " 57%|██████████████████████▏                | 1700/2980 [17:06<40:18,  1.89s/it][INFO|trainer.py:3819] 2024-08-27 18:26:21,222 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:26:21,222 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:26:21,222 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.29it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.26it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.05it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.58it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.34it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.06it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.15it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.16it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.05it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.97it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.04it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.01it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.90it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.96it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.97it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.91it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.97it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.01it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.93it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.03it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.95it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.84it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.82it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.89it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.86it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.91it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.88it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.79it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.78it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.82it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.96it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.88it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.85it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.95it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.02it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.07it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.14it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.09it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.51it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.54it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.65it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.77it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.95it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.88it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.022434843704104424, 'eval_runtime': 17.1576, 'eval_samples_per_second': 123.211, 'eval_steps_per_second': 3.905, 'epoch': 5.7}\n",
      " 57%|██████████████████████▏                | 1700/2980 [17:23<40:18,  1.89s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.06it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:26:38,380 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1700\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:26:38,410 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:26:38,411 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:26:38,597 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:26:38,597 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1700/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0024, 'grad_norm': 0.333234041929245, 'learning_rate': 4.19618829260674e-05, 'epoch': 5.74}\n",
      "{'loss': 0.0028, 'grad_norm': 0.2744249701499939, 'learning_rate': 4.1414750374491554e-05, 'epoch': 5.77}\n",
      "{'loss': 0.0015, 'grad_norm': 0.22402457892894745, 'learning_rate': 4.086867505047874e-05, 'epoch': 5.81}\n",
      "{'loss': 0.0018, 'grad_norm': 0.07419449090957642, 'learning_rate': 4.032372420028377e-05, 'epoch': 5.84}\n",
      "{'loss': 0.0033, 'grad_norm': 0.17648009955883026, 'learning_rate': 3.9779964931688554e-05, 'epoch': 5.87}\n",
      "{'loss': 0.0018, 'grad_norm': 0.28779441118240356, 'learning_rate': 3.923746420573809e-05, 'epoch': 5.91}\n",
      "{'loss': 0.0023, 'grad_norm': 0.025287358090281487, 'learning_rate': 3.8696288828494525e-05, 'epoch': 5.94}\n",
      "{'loss': 0.0018, 'grad_norm': 0.11106324940919876, 'learning_rate': 3.8156505442810515e-05, 'epoch': 5.97}\n",
      "{'loss': 0.0021, 'grad_norm': 0.025583138689398766, 'learning_rate': 3.761818052012228e-05, 'epoch': 6.01}\n",
      "{'loss': 0.0024, 'grad_norm': 0.6729352474212646, 'learning_rate': 3.7081380352264224e-05, 'epoch': 6.04}\n",
      " 60%|███████████████████████▌               | 1800/2980 [20:33<37:10,  1.89s/it][INFO|trainer.py:3819] 2024-08-27 18:29:48,465 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:29:48,466 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:29:48,466 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.39it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.30it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.06it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.59it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.35it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.06it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.15it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.16it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.04it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.95it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.03it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.00it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.88it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.01it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.01it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.95it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.97it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.90it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.96it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.00it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.94it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.04it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.95it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.85it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.83it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.89it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.85it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.90it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.87it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.79it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.83it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.87it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.84it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.92it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.01it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.05it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.92it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.13it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.09it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.61it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.48it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.52it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.63it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.69it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.93it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.88it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.020720509812235832, 'eval_runtime': 17.1814, 'eval_samples_per_second': 123.04, 'eval_steps_per_second': 3.9, 'epoch': 6.04}\n",
      " 60%|███████████████████████▌               | 1800/2980 [20:50<37:10,  1.89s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.06it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:30:05,647 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1800\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:30:05,680 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:30:05,681 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:30:05,870 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:30:05,870 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1800/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0007, 'grad_norm': 0.0203305222094059, 'learning_rate': 3.654617104330528e-05, 'epoch': 6.07}\n",
      "{'loss': 0.0007, 'grad_norm': 0.014013337902724743, 'learning_rate': 3.601261850140866e-05, 'epoch': 6.11}\n",
      "{'loss': 0.0005, 'grad_norm': 0.051690421998500824, 'learning_rate': 3.5480788430715564e-05, 'epoch': 6.14}\n",
      "{'loss': 0.0002, 'grad_norm': 0.11138958483934402, 'learning_rate': 3.495074632325407e-05, 'epoch': 6.17}\n",
      "{'loss': 0.001, 'grad_norm': 0.035890527069568634, 'learning_rate': 3.442255745087417e-05, 'epoch': 6.21}\n",
      "{'loss': 0.0014, 'grad_norm': 0.0006839231355115771, 'learning_rate': 3.3896286857209836e-05, 'epoch': 6.24}\n",
      "{'loss': 0.0006, 'grad_norm': 0.19380293786525726, 'learning_rate': 3.3371999349669325e-05, 'epoch': 6.28}\n",
      "{'loss': 0.0004, 'grad_norm': 0.7674084305763245, 'learning_rate': 3.284975949145438e-05, 'epoch': 6.31}\n",
      "{'loss': 0.0006, 'grad_norm': 0.050598207861185074, 'learning_rate': 3.232963159360975e-05, 'epoch': 6.34}\n",
      "{'loss': 0.0002, 'grad_norm': 9.703935211291537e-05, 'learning_rate': 3.181167970710356e-05, 'epoch': 6.38}\n",
      " 64%|████████████████████████▊              | 1900/2980 [24:02<35:24,  1.97s/it][INFO|trainer.py:3819] 2024-08-27 18:33:17,854 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:33:17,855 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:33:17,855 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.41it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.30it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.08it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.59it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.35it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.06it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.15it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.17it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.05it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.97it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.04it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.00it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.89it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.94it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.96it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.97it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.90it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.96it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.00it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.94it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.03it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.95it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.85it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.82it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.89it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.85it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.90it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.88it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.83it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.95it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.88it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.85it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.95it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.02it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.07it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.14it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.10it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.51it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.51it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.62it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.68it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.75it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.93it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.87it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.03527429699897766, 'eval_runtime': 17.1672, 'eval_samples_per_second': 123.142, 'eval_steps_per_second': 3.903, 'epoch': 6.38}\n",
      " 64%|████████████████████████▊              | 1900/2980 [24:19<35:24,  1.97s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.05it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:33:35,026 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1900\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:33:35,065 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:33:35,066 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:33:35,217 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:33:35,218 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1900/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.002, 'grad_norm': 0.6020278334617615, 'learning_rate': 3.129596761493969e-05, 'epoch': 6.41}\n",
      "{'loss': 0.0015, 'grad_norm': 0.2502550482749939, 'learning_rate': 2.7258462817608516e-05, 'epoch': 6.68}\n",
      "{'loss': 0.0011, 'grad_norm': 0.11989345401525497, 'learning_rate': 2.6765731035409797e-05, 'epoch': 6.71}\n",
      " 67%|██████████████████████████▏            | 2000/2980 [27:29<32:01,  1.96s/it][INFO|trainer.py:3819] 2024-08-27 18:36:44,613 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:36:44,614 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:36:44,614 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.40it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.25it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.04it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.58it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.34it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.06it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.15it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.16it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.04it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.96it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.03it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.00it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.89it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.96it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.98it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.90it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.95it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  3.99it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.93it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.03it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.95it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.83it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.81it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.87it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.85it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.90it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.88it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.83it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.84it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.89it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.93it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.92it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.87it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:12<00:04,  3.87it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.84it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.94it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:13<00:03,  4.02it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.06it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.91it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.11it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.07it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.59it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.64it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.50it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.53it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.64it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.69it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.94it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.88it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.02533048577606678, 'eval_runtime': 17.1868, 'eval_samples_per_second': 123.001, 'eval_steps_per_second': 3.898, 'epoch': 6.71}\n",
      " 67%|██████████████████████████▏            | 2000/2980 [27:46<32:01,  1.96s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.06it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:37:01,801 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2000\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:37:01,831 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:37:01,832 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:37:02,010 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:37:02,010 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2000/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0006, 'grad_norm': 0.1690463423728943, 'learning_rate': 2.6275860429138144e-05, 'epoch': 6.74}\n",
      "{'loss': 0.0004, 'grad_norm': 0.04786922410130501, 'learning_rate': 2.5788911323738035e-05, 'epoch': 6.78}\n",
      "{'loss': 0.0005, 'grad_norm': 0.11290159821510315, 'learning_rate': 2.530494368438683e-05, 'epoch': 6.81}\n",
      "{'loss': 0.0009, 'grad_norm': 0.08898349851369858, 'learning_rate': 2.4824017109110215e-05, 'epoch': 6.85}\n",
      "{'loss': 0.0005, 'grad_norm': 0.005371907725930214, 'learning_rate': 2.434619082144312e-05, 'epoch': 6.88}\n",
      "{'loss': 0.0003, 'grad_norm': 0.01629197783768177, 'learning_rate': 2.3871523663136687e-05, 'epoch': 6.91}\n",
      "{'loss': 0.0001, 'grad_norm': 0.006041764747351408, 'learning_rate': 2.340007408691212e-05, 'epoch': 6.95}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0017633173847571015, 'learning_rate': 2.2931900149262635e-05, 'epoch': 6.98}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0029177425894886255, 'learning_rate': 2.246705950330405e-05, 'epoch': 7.01}\n",
      "{'loss': 0.0008, 'grad_norm': 0.11424438655376434, 'learning_rate': 2.2005609391675197e-05, 'epoch': 7.05}\n",
      " 70%|███████████████████████████▍           | 2100/2980 [30:57<26:52,  1.83s/it][INFO|trainer.py:3819] 2024-08-27 18:40:12,303 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:40:12,303 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:40:12,303 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.45it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.30it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.08it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.59it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.35it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.07it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.15it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.16it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.05it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.97it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.04it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.00it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.88it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.01it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.93it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.92it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.98it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.93it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.81it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.84it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.90it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.97it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.91it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.97it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  3.99it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.93it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.03it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.95it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.84it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.83it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.89it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.86it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.91it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.87it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.88it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.80it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.95it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.84it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.88it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.85it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.95it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.99it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.03it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.07it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.95it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.15it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.10it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.51it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.55it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.65it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.77it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.95it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.89it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.03226489946246147, 'eval_runtime': 17.1572, 'eval_samples_per_second': 123.214, 'eval_steps_per_second': 3.905, 'epoch': 7.05}\n",
      " 70%|███████████████████████████▍           | 2100/2980 [31:14<26:52,  1.83s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:40:29,461 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2100\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:40:29,490 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:40:29,490 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:40:29,660 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:40:29,660 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2100/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0001, 'grad_norm': 0.0006968988454900682, 'learning_rate': 2.1547606639488706e-05, 'epoch': 7.08}\n",
      "{'loss': 0.001, 'grad_norm': 0.0015237192856147885, 'learning_rate': 2.109310764733331e-05, 'epoch': 7.11}\n",
      "{'loss': 0.0001, 'grad_norm': 0.009964353404939175, 'learning_rate': 2.0642168384328524e-05, 'epoch': 7.15}\n",
      "{'loss': 0.0002, 'grad_norm': 0.01901566982269287, 'learning_rate': 2.0194844381232147e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0011767979012802243, 'learning_rate': 1.9751190723602138e-05, 'epoch': 7.21}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0027869022451341152, 'learning_rate': 1.9311262045013074e-05, 'epoch': 7.25}\n",
      "{'loss': 0.0006, 'grad_norm': 0.013782374560832977, 'learning_rate': 1.887511252032817e-05, 'epoch': 7.28}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0015021086437627673, 'learning_rate': 1.844279585902819e-05, 'epoch': 7.32}\n",
      "{'loss': 0.0002, 'grad_norm': 0.032747797667980194, 'learning_rate': 1.8014365298597192e-05, 'epoch': 7.35}\n",
      "{'loss': 0.0004, 'grad_norm': 0.003267277730628848, 'learning_rate': 1.7589873597966726e-05, 'epoch': 7.38}\n",
      " 74%|████████████████████████████▊          | 2200/2980 [34:25<25:12,  1.94s/it][INFO|trainer.py:3819] 2024-08-27 18:43:40,416 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:43:40,417 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:43:40,417 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.47it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.31it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.09it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:00<00:13,  4.60it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.36it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.07it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.16it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.17it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.05it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.97it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.03it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  3.96it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.87it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.00it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.93it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.92it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.98it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.96it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.98it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.91it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.96it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.00it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.94it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.03it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.96it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.85it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.83it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.89it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.85it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.91it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.87it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.79it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.82it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.84it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.88it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.86it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.95it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.02it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.07it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.15it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.09it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.51it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.54it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.65it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.77it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.94it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.88it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.0286441370844841, 'eval_runtime': 17.1517, 'eval_samples_per_second': 123.253, 'eval_steps_per_second': 3.906, 'epoch': 7.38}\n",
      " 74%|████████████████████████████▊          | 2200/2980 [34:42<25:12,  1.94s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:43:57,569 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2200\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:43:57,599 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:43:57,600 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:43:57,787 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:43:57,787 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2200/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0, 'grad_norm': 0.0017046899301931262, 'learning_rate': 1.7169373031018816e-05, 'epoch': 7.42}\n",
      "{'loss': 0.0002, 'grad_norm': 0.04020291939377785, 'learning_rate': 1.675291538014877e-05, 'epoch': 7.45}\n",
      "{'loss': 0.0001, 'grad_norm': 0.009578485041856766, 'learning_rate': 1.6340551929888365e-05, 'epoch': 7.48}\n",
      "{'loss': 0.0002, 'grad_norm': 0.032106589525938034, 'learning_rate': 1.593233346059046e-05, 'epoch': 7.52}\n",
      "{'loss': 0.0001, 'grad_norm': 0.011627127416431904, 'learning_rate': 1.552831024217577e-05, 'epoch': 7.55}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0004844034556299448, 'learning_rate': 1.282172739021013e-05, 'epoch': 7.79}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0017607859335839748, 'learning_rate': 1.2453012631019068e-05, 'epoch': 7.82}\n",
      "{'loss': 0.0002, 'grad_norm': 0.00072768225800246, 'learning_rate': 1.2088921582458101e-05, 'epoch': 7.85}\n",
      "{'loss': 0.0001, 'grad_norm': 0.008527894504368305, 'learning_rate': 1.1729499080392076e-05, 'epoch': 7.89}\n",
      "{'loss': 0.0006, 'grad_norm': 0.000716733280569315, 'learning_rate': 1.1374789385779316e-05, 'epoch': 7.92}\n",
      "{'loss': 0.0003, 'grad_norm': 0.001104805269278586, 'learning_rate': 1.1024836179221153e-05, 'epoch': 7.95}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0003925802593585104, 'learning_rate': 1.0679682555582831e-05, 'epoch': 7.99}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0009624184458516538, 'learning_rate': 1.0339371018686628e-05, 'epoch': 8.02}\n",
      "{'loss': 0.0, 'grad_norm': 0.006597503554075956, 'learning_rate': 1.0003943476077743e-05, 'epoch': 8.05}\n",
      " 81%|███████████████████████████████▍       | 2400/2980 [41:22<18:43,  1.94s/it][INFO|trainer.py:3819] 2024-08-27 18:50:37,217 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:50:37,217 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:50:37,217 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.40it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.30it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.07it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.59it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.35it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.07it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.16it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.18it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.05it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.97it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.04it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.01it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.89it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.96it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.97it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.91it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.97it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.00it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.91it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.02it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.94it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.84it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.83it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.89it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.85it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.90it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.88it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.83it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.98it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.88it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.84it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.94it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.02it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.06it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.15it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.05it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.58it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.64it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.49it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.53it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.64it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.69it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.94it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.88it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.03320963680744171, 'eval_runtime': 17.1594, 'eval_samples_per_second': 123.198, 'eval_steps_per_second': 3.905, 'epoch': 8.05}\n",
      " 81%|███████████████████████████████▍       | 2400/2980 [41:39<18:43,  1.94s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:50:54,378 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2400\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:50:54,407 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:50:54,408 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:50:54,563 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:50:54,563 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2400/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0001, 'grad_norm': 0.007516131736338139, 'learning_rate': 9.673441233863662e-06, 'epoch': 8.09}\n",
      "{'loss': 0.0, 'grad_norm': 0.002192088635638356, 'learning_rate': 9.347904991627382e-06, 'epoch': 8.12}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00044440015335567296, 'learning_rate': 9.027374837415637e-06, 'epoch': 8.15}\n",
      "{'loss': 0.0, 'grad_norm': 0.0009003145387396216, 'learning_rate': 8.711890242802218e-06, 'epoch': 8.19}\n",
      "{'loss': 0.0, 'grad_norm': 0.0015278232749551535, 'learning_rate': 8.401490058027151e-06, 'epoch': 8.22}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00011122752766823396, 'learning_rate': 8.096212507212697e-06, 'epoch': 8.26}\n",
      "{'loss': 0.0001, 'grad_norm': 0.004042509477585554, 'learning_rate': 7.796095183656077e-06, 'epoch': 8.29}\n",
      "{'loss': 0.0, 'grad_norm': 0.0004277436819393188, 'learning_rate': 7.501175045200159e-06, 'epoch': 8.32}\n",
      "{'loss': 0.0001, 'grad_norm': 9.785875590750948e-05, 'learning_rate': 7.21148840968226e-06, 'epoch': 8.36}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0004356513964012265, 'learning_rate': 6.927070950461834e-06, 'epoch': 8.39}\n",
      " 84%|████████████████████████████████▋      | 2500/2980 [44:49<15:24,  1.93s/it][INFO|trainer.py:3819] 2024-08-27 18:54:04,904 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:54:04,905 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:54:04,905 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.40it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.29it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.07it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.53it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.31it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.04it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.12it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:14,  4.14it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.03it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.95it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.02it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.00it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.89it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.95it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.97it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.91it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.97it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.01it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.94it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.03it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.94it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.83it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.81it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.88it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.85it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.90it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.88it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.82it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.84it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.93it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.93it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:12<00:04,  3.88it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.85it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.94it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:13<00:03,  4.02it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.07it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.93it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.14it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.09it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.50it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.53it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.64it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.69it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.75it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.93it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.87it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.03511812537908554, 'eval_runtime': 17.1827, 'eval_samples_per_second': 123.03, 'eval_steps_per_second': 3.899, 'epoch': 8.39}\n",
      " 84%|████████████████████████████████▋      | 2500/2980 [45:06<15:24,  1.93s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.06it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:54:22,090 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2500\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:54:22,119 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:54:22,120 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:54:22,277 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:54:22,277 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2500/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0001, 'grad_norm': 0.0004201502015348524, 'learning_rate': 6.6479576920274235e-06, 'epoch': 8.42}\n",
      "{'loss': 0.0, 'grad_norm': 0.002687556203454733, 'learning_rate': 6.374183005683626e-06, 'epoch': 8.46}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0003568703541532159, 'learning_rate': 6.1057806053185e-06, 'epoch': 8.49}\n",
      "{'loss': 0.0, 'grad_norm': 0.0001056490364135243, 'learning_rate': 5.842783543251734e-06, 'epoch': 8.52}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0014142801519483328, 'learning_rate': 5.5852242061646025e-06, 'epoch': 8.56}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0032588746398687363, 'learning_rate': 5.3331343111115986e-06, 'epoch': 8.59}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022740886197425425, 'learning_rate': 5.086544901614687e-06, 'epoch': 8.62}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010491417197044939, 'learning_rate': 4.845486343840494e-06, 'epoch': 8.66}\n",
      "{'loss': 0.0, 'grad_norm': 0.003891482250764966, 'learning_rate': 4.609988322860814e-06, 'epoch': 8.69}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0007865584339015186, 'learning_rate': 4.380079838997086e-06, 'epoch': 8.72}\n",
      " 87%|██████████████████████████████████     | 2600/2980 [48:17<12:08,  1.92s/it][INFO|trainer.py:3819] 2024-08-27 18:57:32,717 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 18:57:32,717 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 18:57:32,717 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.44it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.31it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.08it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.55it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.33it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.04it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.14it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.15it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.04it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.96it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.04it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.01it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.89it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.98it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.94it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.81it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.84it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.96it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.91it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.89it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.96it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  3.99it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.93it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.02it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.94it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.84it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.82it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.89it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.86it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.91it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.87it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.83it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.84it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.93it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.96it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.93it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:12<00:04,  3.87it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.84it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.94it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.93it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:13<00:03,  3.99it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.04it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.92it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.13it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.09it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.51it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.54it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.65it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.77it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.95it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.89it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.03610739856958389, 'eval_runtime': 17.1805, 'eval_samples_per_second': 123.046, 'eval_steps_per_second': 3.9, 'epoch': 8.72}\n",
      " 87%|██████████████████████████████████     | 2600/2980 [48:34<12:08,  1.92s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 18:57:49,898 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2600\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 18:57:49,925 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 18:57:49,926 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 18:57:50,083 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 18:57:50,083 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2600/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0011, 'grad_norm': 5.3183695854386315e-05, 'learning_rate': 4.155789204249155e-06, 'epoch': 8.76}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00026392971631139517, 'learning_rate': 3.9371440388088265e-06, 'epoch': 8.79}\n",
      "{'loss': 0.0011, 'grad_norm': 0.09159781783819199, 'learning_rate': 3.7241712676585274e-06, 'epoch': 8.83}\n",
      "{'loss': 0.0001, 'grad_norm': 0.005763828754425049, 'learning_rate': 3.5168971172556906e-06, 'epoch': 8.86}\n",
      "{'loss': 0.0003, 'grad_norm': 0.08215156197547913, 'learning_rate': 3.3153471123030964e-06, 'epoch': 8.89}\n",
      "{'loss': 0.0005, 'grad_norm': 0.07520855963230133, 'learning_rate': 3.119546072605617e-06, 'epoch': 8.93}\n",
      "{'loss': 0.0002, 'grad_norm': 0.000537549436558038, 'learning_rate': 2.9295181100138235e-06, 'epoch': 8.96}\n",
      "{'loss': 0.0, 'grad_norm': 0.0038528337609022856, 'learning_rate': 2.7452866254547126e-06, 'epoch': 8.99}\n",
      "{'loss': 0.0, 'grad_norm': 8.264237112598494e-05, 'learning_rate': 2.5668743060500432e-06, 'epoch': 9.03}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0026726778596639633, 'learning_rate': 2.3943031223224775e-06, 'epoch': 9.06}\n",
      " 91%|███████████████████████████████████▎   | 2700/2980 [51:45<09:01,  1.93s/it][INFO|trainer.py:3819] 2024-08-27 19:01:00,604 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 19:01:00,604 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 19:01:00,604 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.44it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.31it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.08it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:00<00:13,  4.60it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:13,  4.36it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.07it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.16it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.16it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.05it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.97it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.04it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.00it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.89it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.01it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.02it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.96it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.82it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.97it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.90it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.97it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.01it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.94it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.04it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.96it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.86it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.83it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.90it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.86it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.91it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.87it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.83it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.95it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:11<00:04,  3.88it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.85it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.95it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.02it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.06it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.15it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.09it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.50it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.54it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.64it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.69it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.95it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.88it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.034807927906513214, 'eval_runtime': 17.1517, 'eval_samples_per_second': 123.253, 'eval_steps_per_second': 3.906, 'epoch': 9.06}\n",
      " 91%|███████████████████████████████████▎   | 2700/2980 [52:02<09:01,  1.93s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.05it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 19:01:17,756 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2700\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 19:01:17,783 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 19:01:17,784 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 19:01:17,937 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 19:01:17,937 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2700/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0002, 'grad_norm': 0.0008525270968675613, 'learning_rate': 2.2275943254901065e-06, 'epoch': 9.09}\n",
      "{'loss': 0.0003, 'grad_norm': 0.004588547628372908, 'learning_rate': 2.066768444849432e-06, 'epoch': 9.13}\n",
      "{'loss': 0.0003, 'grad_norm': 0.08138010650873184, 'learning_rate': 1.9118452852472603e-06, 'epoch': 9.16}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0022998182103037834, 'learning_rate': 1.762843924641927e-06, 'epoch': 9.19}\n",
      "{'loss': 0.0001, 'grad_norm': 0.000524010683875531, 'learning_rate': 1.6197827117538754e-06, 'epoch': 9.23}\n",
      "{'loss': 0.0, 'grad_norm': 0.003998046740889549, 'learning_rate': 1.4826792638061414e-06, 'epoch': 9.26}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0016013117274269462, 'learning_rate': 1.3515504643548782e-06, 'epoch': 9.3}\n",
      "{'loss': 0.0004, 'grad_norm': 0.07993225753307343, 'learning_rate': 1.2264124612102412e-06, 'epoch': 9.33}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00026309562963433564, 'learning_rate': 1.1072806644478739e-06, 'epoch': 9.36}\n",
      "{'loss': 0.0004, 'grad_norm': 0.03190355375409126, 'learning_rate': 9.941697445112196e-07, 'epoch': 9.4}\n",
      " 94%|████████████████████████████████████▋  | 2800/2980 [55:13<05:38,  1.88s/it][INFO|trainer.py:3819] 2024-08-27 19:04:28,756 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3821] 2024-08-27 19:04:28,756 >>   Num examples = 2114\n",
      "[INFO|trainer.py:3824] 2024-08-27 19:04:28,756 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                          | 2/67 [00:00<00:08,  7.43it/s]\u001b[A\n",
      "  4%|█▉                                          | 3/67 [00:00<00:12,  5.31it/s]\u001b[A\n",
      "  6%|██▋                                         | 4/67 [00:00<00:12,  5.09it/s]\u001b[A\n",
      "  7%|███▎                                        | 5/67 [00:01<00:13,  4.60it/s]\u001b[A\n",
      "  9%|███▉                                        | 6/67 [00:01<00:14,  4.35it/s]\u001b[A\n",
      " 10%|████▌                                       | 7/67 [00:01<00:14,  4.07it/s]\u001b[A\n",
      " 12%|█████▎                                      | 8/67 [00:01<00:14,  4.15it/s]\u001b[A\n",
      " 13%|█████▉                                      | 9/67 [00:02<00:13,  4.16it/s]\u001b[A\n",
      " 15%|██████▍                                    | 10/67 [00:02<00:14,  4.03it/s]\u001b[A\n",
      " 16%|███████                                    | 11/67 [00:02<00:14,  3.95it/s]\u001b[A\n",
      " 18%|███████▋                                   | 12/67 [00:02<00:13,  4.02it/s]\u001b[A\n",
      " 19%|████████▎                                  | 13/67 [00:03<00:13,  4.00it/s]\u001b[A\n",
      " 21%|████████▉                                  | 14/67 [00:03<00:13,  3.90it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 15/67 [00:03<00:12,  4.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 16/67 [00:03<00:12,  3.94it/s]\u001b[A\n",
      " 25%|██████████▉                                | 17/67 [00:04<00:12,  3.93it/s]\u001b[A\n",
      " 27%|███████████▌                               | 18/67 [00:04<00:12,  3.99it/s]\u001b[A\n",
      " 28%|████████████▏                              | 19/67 [00:04<00:11,  4.01it/s]\u001b[A\n",
      " 30%|████████████▊                              | 20/67 [00:04<00:11,  3.95it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 21/67 [00:05<00:12,  3.83it/s]\u001b[A\n",
      " 33%|██████████████                             | 22/67 [00:05<00:11,  3.85it/s]\u001b[A\n",
      " 34%|██████████████▊                            | 23/67 [00:05<00:11,  3.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 24/67 [00:05<00:10,  3.97it/s]\u001b[A\n",
      " 37%|████████████████                           | 25/67 [00:06<00:10,  3.92it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 26/67 [00:06<00:10,  3.90it/s]\u001b[A\n",
      " 40%|█████████████████▎                         | 27/67 [00:06<00:10,  3.97it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 28/67 [00:06<00:09,  4.00it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 29/67 [00:07<00:09,  3.94it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 30/67 [00:07<00:09,  4.03it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 31/67 [00:07<00:09,  3.95it/s]\u001b[A\n",
      " 48%|████████████████████▌                      | 32/67 [00:07<00:09,  3.85it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 33/67 [00:08<00:08,  3.83it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 34/67 [00:08<00:08,  3.89it/s]\u001b[A\n",
      " 52%|██████████████████████▍                    | 35/67 [00:08<00:08,  3.86it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 36/67 [00:08<00:07,  3.91it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 37/67 [00:09<00:07,  3.86it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 38/67 [00:09<00:07,  3.88it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 39/67 [00:09<00:07,  3.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▋                 | 40/67 [00:09<00:07,  3.79it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 41/67 [00:10<00:06,  3.81it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 42/67 [00:10<00:06,  3.93it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 43/67 [00:10<00:06,  3.82it/s]\u001b[A\n",
      " 66%|████████████████████████████▏              | 44/67 [00:10<00:05,  3.92it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 45/67 [00:11<00:05,  3.95it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 46/67 [00:11<00:05,  3.93it/s]\u001b[A\n",
      " 70%|██████████████████████████████▏            | 47/67 [00:11<00:05,  3.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 48/67 [00:12<00:04,  3.87it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 49/67 [00:12<00:04,  3.84it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 50/67 [00:12<00:04,  3.94it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 51/67 [00:12<00:04,  3.97it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 52/67 [00:12<00:03,  4.02it/s]\u001b[A\n",
      " 79%|██████████████████████████████████         | 53/67 [00:13<00:03,  4.06it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▋        | 54/67 [00:13<00:03,  3.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 55/67 [00:13<00:02,  4.14it/s]\u001b[A\n",
      " 84%|███████████████████████████████████▉       | 56/67 [00:13<00:02,  4.09it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 57/67 [00:14<00:02,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 58/67 [00:14<00:02,  3.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 59/67 [00:14<00:02,  3.51it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 60/67 [00:15<00:01,  3.55it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 61/67 [00:15<00:01,  3.65it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▊   | 62/67 [00:15<00:01,  3.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 63/67 [00:15<00:01,  3.77it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 64/67 [00:16<00:00,  3.95it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▋ | 65/67 [00:16<00:00,  3.88it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▎| 66/67 [00:16<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.03496210649609566, 'eval_runtime': 17.166, 'eval_samples_per_second': 123.151, 'eval_steps_per_second': 3.903, 'epoch': 9.4}\n",
      " 94%|████████████████████████████████████▋  | 2800/2980 [55:30<05:38,  1.88s/it]\n",
      "100%|███████████████████████████████████████████| 67/67 [00:16<00:00,  4.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3503] 2024-08-27 19:04:45,926 >> Saving model checkpoint to /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2800\n",
      "[INFO|configuration_utils.py:731] 2024-08-27 19:04:45,953 >> loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-27 19:04:45,954 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-08-27 19:04:46,101 >> tokenizer config file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-08-27 19:04:46,102 >> Special tokens file saved in /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2800/special_tokens_map.json\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "{'loss': 0.0, 'grad_norm': 0.0001625923760002479, 'learning_rate': 8.870936304049726e-07, 'epoch': 9.43}\n",
      "{'loss': 0.0, 'grad_norm': 0.006066893693059683, 'learning_rate': 7.860655079797608e-07, 'epoch': 9.46}\n",
      "{'loss': 0.0005, 'grad_norm': 0.00015984787023626268, 'learning_rate': 6.910978183083838e-07, 'epoch': 9.5}\n",
      "{'loss': 0.0, 'grad_norm': 0.00017546737217344344, 'learning_rate': 6.022022561537887e-07, 'epoch': 9.53}\n",
      "{'loss': 0.0, 'grad_norm': 0.0015357912052422762, 'learning_rate': 5.193897685288829e-07, 'epoch': 9.56}\n",
      "{'loss': 0.0, 'grad_norm': 0.00022345723118633032, 'learning_rate': 4.4267055334850093e-07, 'epoch': 9.6}\n",
      "{'loss': 0.0004, 'grad_norm': 0.035627275705337524, 'learning_rate': 3.72054058173571e-07, 'epoch': 9.63}\n",
      "{'loss': 0.0005, 'grad_norm': 9.067464270628989e-05, 'learning_rate': 3.0754897904769486e-07, 'epoch': 9.66}\n",
      " 97%|█████████████████████████████████████▊ | 2885/2980 [58:11<02:56,  1.85s/it][rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/data2/downloads/LLaMA-Factory/src/llamafactory/launcher.py\", line 23, in <module>\n",
      "[rank1]:     launch()\n",
      "[rank1]:   File \"/data2/downloads/LLaMA-Factory/src/llamafactory/launcher.py\", line 19, in launch\n",
      "[rank1]:     run_exp()\n",
      "[rank1]:   File \"/data2/downloads/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "[rank1]:   File \"/data2/downloads/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 94, in run_sft\n",
      "[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py\", line 1938, in train\n",
      "[rank1]:     return inner_training_loop(\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py\", line 2279, in _inner_training_loop\n",
      "[rank1]:     tr_loss_step = self.training_step(model, inputs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py\", line 3318, in training_step\n",
      "[rank1]:     loss = self.compute_loss(model, inputs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/trainer.py\", line 3363, in compute_loss\n",
      "[rank1]:     outputs = model(**inputs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "[rank1]:     return forward_call(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\n",
      "[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\n",
      "[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "[rank1]:     return forward_call(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 819, in forward\n",
      "[rank1]:     return model_forward(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 807, in __call__\n",
      "[rank1]:     return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/peft/peft_model.py\", line 1577, in forward\n",
      "[rank1]:     return self.base_model(\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "[rank1]:     return forward_call(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 188, in forward\n",
      "[rank1]:     return self.model.forward(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1082, in forward\n",
      "[rank1]:     loss = loss_fct(shift_logits, shift_labels)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "[rank1]:     return forward_call(*args, **kwargs)\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1188, in forward\n",
      "[rank1]:     return F.cross_entropy(input, target, weight=self.weight,\n",
      "[rank1]:   File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/nn/functional.py\", line 3104, in cross_entropy\n",
      "[rank1]:     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.77 GiB. GPU 1 has a total capacity of 23.69 GiB of which 4.60 GiB is free. Including non-PyTorch memory, this process has 19.09 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 4.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "W0827 19:07:29.612000 139932174230464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 12403 closing signal SIGTERM\n",
      "W0827 19:07:29.615000 139932174230464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 12405 closing signal SIGTERM\n",
      "W0827 19:07:29.615000 139932174230464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 12407 closing signal SIGTERM\n",
      "E0827 19:07:30.144000 139932174230464 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 12404) of binary: /data2/anaconda3/envs/python3_10/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/data2/anaconda3/envs/python3_10/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
      "    run(args)\n",
      "  File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
      "    elastic_launch(\n",
      "  File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 133, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/data2/downloads/LLaMA-Factory/src/llamafactory/launcher.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-08-27_19:07:29\n",
      "  host      : ubuntu\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 12404)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train /data2/downloads/LLaMA-Factory/qwen2_lora_sft.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9347991e-98f4-4d80-b7d7-90a8be524a2b",
   "metadata": {},
   "source": [
    "## 验证数据集上评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b117369-c7f0-4903-b44d-472af97d83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run evaluate.py\n",
    "testdata_path = '/data2/anti_fraud/dataset/eval0819.jsonl'\n",
    "model_path = '/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f0eef-37dd-46c5-98fc-8d1da34ee396",
   "metadata": {},
   "source": [
    "分别评估验证数据集上在不同checkpoint上的性能表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9019e1-0f49-41b9-9cb1-0cda54b96fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:22<00:00, 11.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1160, fp:5, fn:103, tp:1080\n",
      "precision: 0.9953917050691244, recall: 0.9129332206255283\n",
      "CPU times: user 3min 26s, sys: 21.7 s, total: 3min 48s\n",
      "Wall time: 3min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## eval_loss=0.0152\n",
    "checkpoint_path_900 = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-900'\n",
    "evaluate(model_path, checkpoint_path_900, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7679687-65e3-4185-918b-cfd95169b84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:13<00:00, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1162, fp:3, fn:67, tp:1116\n",
      "precision: 0.9973190348525469, recall: 0.9433643279797126\n",
      "CPU times: user 3min 19s, sys: 19.3 s, total: 3min 38s\n",
      "Wall time: 3min 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## eval_loss=0.0137\n",
    "checkpoint_path_1400 = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1400'\n",
    "evaluate(model_path, checkpoint_path_1400, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c88dcc-dafa-495a-9b78-b1f6dcd9adf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:22<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1161, fp:4, fn:17, tp:1166\n",
      "precision: 0.9965811965811966, recall: 0.9856297548605241\n",
      "CPU times: user 3min 23s, sys: 21.7 s, total: 3min 45s\n",
      "Wall time: 3min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## eval_loss=0.020\n",
    "checkpoint_path_1800 = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1800'\n",
    "evaluate(model_path, checkpoint_path_1800, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52529bfd-6cd2-4d68-b329-923aa01fd283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:19<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1161, fp:4, fn:9, tp:1174\n",
      "precision: 0.9966044142614601, recall: 0.9923922231614539\n",
      "CPU times: user 3min 21s, sys: 21.9 s, total: 3min 43s\n",
      "Wall time: 3min 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## eval_loss=0.035\n",
    "checkpoint_path_2800 = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2800'\n",
    "evaluate(model_path, checkpoint_path_2800, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d6e0b-0863-4a07-869e-35da662956a2",
   "metadata": {},
   "source": [
    "从验证数据集上的评估结果来看，模型的精确率和召回率都有了显著的提升，多卡训练效果显著好于单卡，应该是批量增大，训练更稳定带来的好处。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c115e3d-28b1-426d-adc4-b63f940b7bf3",
   "metadata": {},
   "source": [
    "## 测试数据集上评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070510d1-f2f1-4833-9f0d-ca50ecd1d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run evaluate.py\n",
    "testdata_path = '/data2/anti_fraud/dataset/test0819.jsonl'\n",
    "model_path = '/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct'\n",
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9242b5-e3a1-4b2c-be9d-5a008ceb5743",
   "metadata": {},
   "source": [
    "分别评估不同checkpoint在测试数据集上的性能表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f0c6c2-4c62-435e-8b06-997a53949a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2349/2349 [03:43<00:00, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1142, fp:25, fn:171, tp:1011\n",
      "precision: 0.9758687258687259, recall: 0.8553299492385786\n",
      "CPU times: user 3min 44s, sys: 25.3 s, total: 4min 10s\n",
      "Wall time: 3min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## eval_loss=0.0152\n",
    "checkpoint_path_900 = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-900'\n",
    "evaluate(model_path, checkpoint_path_900, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e49efdb-4957-486b-bd40-7a0eb34599e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2349/2349 [03:22<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1136, fp:31, fn:162, tp:1020\n",
      "precision: 0.9705042816365367, recall: 0.8629441624365483\n",
      "CPU times: user 3min 26s, sys: 19.6 s, total: 3min 46s\n",
      "Wall time: 3min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## eval_loss=0.0137\n",
    "checkpoint_path_1400 = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1400'\n",
    "evaluate(model_path, checkpoint_path_1400, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d199cd0-061d-435e-b01e-f80bbc821506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2349/2349 [03:29<00:00, 11.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1129, fp:38, fn:104, tp:1078\n",
      "precision: 0.9659498207885304, recall: 0.9120135363790186\n",
      "CPU times: user 3min 29s, sys: 23.8 s, total: 3min 52s\n",
      "Wall time: 3min 32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## eval_loss=0.020\n",
    "checkpoint_path_1800 = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1800'\n",
    "evaluate(model_path, checkpoint_path_1800, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f85f3a-0fbc-4431-871a-16082a3315c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2349/2349 [03:50<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1112, fp:55, fn:69, tp:1113\n",
      "precision: 0.9529109589041096, recall: 0.9416243654822335\n",
      "CPU times: user 3min 43s, sys: 31.6 s, total: 4min 15s\n",
      "Wall time: 3min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## eval_loss=0.035\n",
    "checkpoint_path_2800 = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-2800'\n",
    "evaluate(model_path, checkpoint_path_2800, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a204bb-d76b-4d89-a200-abb76bb2ce26",
   "metadata": {},
   "source": [
    "测试数据集上的评测结果相比验证数据集上的评估结果，性能有明显差距，模型训练中应该是出现了过拟合。\n",
    "从相应的损失和梯度数据上也能看出来，在2885步时训练损失已经为0，梯度也变得非常小（0.00016）。\n",
    "```json\n",
    "{'loss': 0.0, 'grad_norm': 0.0001625923760002479, 'learning_rate': 8.870936304049726e-07, 'epoch': 9.43}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
