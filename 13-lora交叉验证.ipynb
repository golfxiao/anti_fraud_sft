{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763a3d71-910f-4128-817d-6724e4638e60",
   "metadata": {},
   "source": [
    "## 引言\n",
    "\n",
    "交叉验证主要讨论的是数据集的划分问题。\n",
    "\n",
    "通常情况下，我们会采用均匀随机抽样的方式将数据集划分成3个部分——训练集、验证集和测试集，这三个集合不能有交集，常见的比例是8:1:1（如同[前文](https://golfxiao.blog.csdn.net/article/details/141325192)我们所作的划分)。这三个数据集的用途分别是：\n",
    "- 训练集：用来训练模型，去学习模型的权重和偏置这些参数，这些参数可称为学习参数。\n",
    "- 验证集：用于在训练过程中选择超参数，比如批量大小、学习率、迭代次数等，它并不参与梯度下降，也不参与学习参数的确定。\n",
    "- 测试集：用于训练完成后评价最终的模型时使用，它既不参与学习参数的确定，也不参数超参数的选择，而仅仅使用于模型的评价。\n",
    "\n",
    "> 注：千万不能在训练过程中使用测试集，不论是用于训练还是用于超参数的选择，这会将测试数据无意中提前透露给模型，相当于作弊，使得模型测试时准确率虚高。\r\n",
    "\n",
    "而交叉验证与上述不同的地方在于：在手动划分时只分出训练集和测试集，在训练时再从训练集中动态抽取一定比例作为验证集，并且在多轮训练中会循环提取不同的训练集和验证集，例如：\n",
    "- 第一轮训练时，将训练集平均分成5份，其中4份用来训练，1份用来验证。\n",
    "- 第二轮训练时，取另外的4份来训练，剩下的1份来验证。\n",
    "- ……\n",
    "- 如此循环，直到每份数据都参与过训练和验证。\n",
    "\n",
    "这样做的好处在于：模型能更充分的利用数据，更全面的学习到数据的整体特征，减少过拟合风险。叉验证的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320ce50-6b33-44cf-9824-a6cb4cef6a66",
   "metadata": {},
   "source": [
    "## 训练过程\n",
    "\n",
    "#### 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "038f21c2-1d67-4085-970a-c154882a5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0236fa35-d30e-4d42-b80a-cae1a40db473",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_path = '/data2/anti_fraud/dataset/train0819.jsonl'\n",
    "evaldata_path = '/data2/anti_fraud/dataset/eval0819.jsonl'\n",
    "model_path = '/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct'\n",
    "output_path = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f534275-1f8b-49ce-bbb7-e4d37e520e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57276e83-035d-41ea-be8b-c59c88ed5322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fc1d82342f41ebb35e579878de0892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b173bd281e24429a2704bdd2ecbb71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "train_dataset, eval_dataset = load_dataset(traindata_path, evaldata_path, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa35d9-ca22-4dc4-b76d-a7cde96bc58e",
   "metadata": {},
   "source": [
    "#### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6612c1d-eff5-4cce-b105-787b50e09682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gc\n",
    "import numpy as np\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35740c33-6a0a-46f2-b961-c3e8c6540ab5",
   "metadata": {},
   "source": [
    "拼接训练集和验证集作为一个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3a7f8569-b568-432a-ae12-ad0510bc3a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21135"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = concatenate_datasets([train_dataset, eval_dataset])\n",
    "len(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b6265-2a54-4ed7-8f1e-7bf5843ae152",
   "metadata": {},
   "source": [
    "创建KFold对象用于按折子划分数据集。\n",
    "- n_splits=5：表示将数据集划分为5份。\n",
    "- shuffle=True：表示调用`kf.split`划分数据集前先将顺序打乱。\n",
    "\n",
    "> KFold是由sklearn库提供的k折交叉验证方法，它通过将数据集分成k个相同大小的子集（称为折），每次迭代数据集时，使用其中一个作为验证集，其余4个作为训练集，并重复这个过程k次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48328376-b60d-4806-82d5-67c16d3395be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=None, shuffle=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "kf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3c6ec-7a16-4b1a-aec0-9f270aaadfed",
   "metadata": {},
   "source": [
    "用kfold划分数据集时，实际拿到的是数据在数据集中的索引顺序，如下面示例的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "017abd90-a5b0-4c90-8c8f-1a4f6996bcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     2,     3, ..., 21129, 21131, 21134]),\n",
       " array([    1,     9,    12, ..., 21130, 21132, 21133]),\n",
       " 16908,\n",
       " 4227)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = kf.split(np.arange(len(datasets)))\n",
    "train_indexes, val_indexes = next(indexes)\n",
    "train_indexes, val_indexes, len(train_indexes), len(val_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db19b3-4f8f-4429-95d8-312c95e24437",
   "metadata": {},
   "source": [
    "#### 超参数定义\n",
    "\n",
    "定义超参构造函数，包括训练参数和Lora微调参数。这里相对于之前作的调整在于：\n",
    "- 修改评估和保存模型的策略，由每100step改为每个epoch，原因是前者保存的checkpoint有太多冗余。\n",
    "- 将num_train_epochs调整为2，表示每个折子的数据集训练2遍，k=5时数据总共会训练10遍。\n",
    "\n",
    "> 注：当`per_device_train_batch_size=16`时训练过程中会意外发生OOM，所以临时将批次大小per_device_train_batch_size改为8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c33964a-c461-4801-a94a-731465fa40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arguments(output_path):\n",
    "    train_args = build_train_arguments(output_path)\n",
    "    train_args.eval_strategy='epoch'\n",
    "    train_args.save_strategy='epoch'\n",
    "    train_args.num_train_epochs = 2\n",
    "    train_args.per_device_train_batch_size = 8\n",
    "    \n",
    "    lora_config = build_loraconfig()\n",
    "    lora_config.lora_dropout = 0.2   # 增加泛化能力\n",
    "    lora_config.r = 16\n",
    "    lora_config.lora_alpha = 32\n",
    "    return train_args, lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31ef6d-b0cc-4cd8-a549-415ebfb0560d",
   "metadata": {},
   "source": [
    "由于训练过程中需要迭代更换不同的训练集和验证集组合，而更换数据集就需要重新创建训练器，传入新的模型实例。除了第一次训练是从0开始训练，后面几次都需要加载前一轮训练保存的最新checkpoint，以接着之前的结果继续训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92178c-2871-4d61-88ae-4eaa0eecbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "定义一个`find_last_checkpoint`方法，用于从一个目录中查找最新的checkpoint。\n",
    " - glob.glob 函数可以在指定目录下查找所有匹配 `checkpoint-*` 模式的子目录\n",
    " - os.path.getctime 返回文件的创建时间（或最近修改时间）\n",
    " - max 函数根据这些时间找出最后创建的目录，也就是最新的checkpoint。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9952aa5f-38a9-41a0-bbb8-f28e800ed34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0830_1/checkpoint-3522'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 确定最后的checkpoint目录\n",
    "def find_last_checkpoint(output_dir):\n",
    "    checkpoint_dirs = glob.glob(os.path.join(output_dir, 'checkpoint-*'))\n",
    "    last_checkpoint_dir = max(checkpoint_dirs, key=os.path.getctime)\n",
    "    return last_checkpoint_dir\n",
    "\n",
    "find_last_checkpoint(\"/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0830_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebeab97-8c1b-4270-ae0e-5778f5432b86",
   "metadata": {},
   "source": [
    "定义一个新的加载模型的方法，用于从基座模型和指定的checkpoint中加载最新训练的模型，并根据训练目标来设置参数的require_grad属性，这里将来自lora的参数都设置为需要梯度，其余参数设置不可训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a4e5d61-49a7-4eb3-9611-d1f731eac77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_v2(model_path, checkpoint_path='', device='cuda'):\n",
    "    # 加载模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).to(device)\n",
    "    # 加载lora权重\n",
    "    if checkpoint_path: \n",
    "        model = PeftModel.from_pretrained(model, model_id=checkpoint_path).to(device)\n",
    "    # 将基础模型的参数设置为不可训练\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # 将 LoRA 插入模块的参数设置为可训练\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09908d7-6512-48b6-b876-04dfe2f9661c",
   "metadata": {},
   "source": [
    "在这个训练过程中，第一次训练用的是从零初始化的微调秩，而后面几次训练则需要从指定checkpoint来初始化微调秩，这导致了[原先的build_trainer方法](https://golfxiao.blog.csdn.net/article/details/141500352)不通用。所以定义一个新的训练器构建方法，将加载微调参数的逻辑移到外面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c12a3082-758e-49d9-9106-2a6e2d285b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trainer_v2(model, tokenizer, train_args, train_dataset, eval_dataset):\n",
    "    # 开启梯度检查点时，要执行该方法\n",
    "    if train_args.gradient_checkpointing:\n",
    "        model.enable_input_require_grads()\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],  # 早停回调\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829169e1-d681-47f2-8c7c-5ed85144bdb5",
   "metadata": {},
   "source": [
    "定义交叉训练的主循环。\n",
    "- kf.split函数划分了5份数据索引，以这5份数据索引进行5次迭代。\n",
    "- 使用`datasets.select`基于索引在每次迭代时选择不同的数据作为训练集和验证集。\n",
    "- 为了避免前次迭代训练的结果被下次迭代的结果给覆盖，每次迭代训练通过fold来拼接不同的输出目录output_path。\n",
    "- 如果存在last_checkpoint_path,则从checkpoint来加载模型，如果不存在，则使用get_peft_model向模型中插入一个新的Lora微调秩。\n",
    "- 使用新的build_trainer_v2方法来构建训练器并开始训练。\n",
    "- 每次迭代完都找出此次训练中最新的checkpoint，作为下次训练的起点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fa085ec-d4dd-4b77-97d4-5238cc79b527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/model.safetensors\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=1 start, train_index=[    0     1     2 ... 21131 21133 21134], val_index=[    3     8    10 ... 21124 21130 21132]\n",
      "train data: 16908, eval: 4227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4226' max='4226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4226/4226 56:04, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.011420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.013666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=1, result = TrainOutput(global_step=4226, training_loss=0.007882393647162119, metrics={'train_runtime': 3364.7696, 'train_samples_per_second': 10.05, 'train_steps_per_second': 1.256, 'total_flos': 8.266749122550989e+16, 'train_loss': 0.007882393647162119, 'epoch': 2.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/model.safetensors\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=2 start, train_index=[    0     1     2 ... 21130 21132 21134], val_index=[    6    22    37 ... 21125 21131 21133]\n",
      "train data: 16908, eval: 4227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4226' max='4226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4226/4226 56:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.004718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.004082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=2, result = TrainOutput(global_step=4226, training_loss=0.004613035453062235, metrics={'train_runtime': 3361.5076, 'train_samples_per_second': 10.06, 'train_steps_per_second': 1.257, 'total_flos': 8.219199150976205e+16, 'train_loss': 0.004613035453062235, 'epoch': 2.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/model.safetensors\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=3 start, train_index=[    0     1     2 ... 21132 21133 21134], val_index=[    5    16    20 ... 21126 21128 21129]\n",
      "train data: 16908, eval: 4227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4226' max='4226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4226/4226 55:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=3, result = TrainOutput(global_step=4226, training_loss=0.003411124254891226, metrics={'train_runtime': 3358.0538, 'train_samples_per_second': 10.07, 'train_steps_per_second': 1.258, 'total_flos': 8.188047700785562e+16, 'train_loss': 0.003411124254891226, 'epoch': 2.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/model.safetensors\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=4 start, train_index=[    2     3     4 ... 21131 21132 21133], val_index=[    0     1     7 ... 21116 21120 21134]\n",
      "train data: 16908, eval: 4227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "loading configuration file /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4226' max='4226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4226/4226 56:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.002273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.002139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data2/anaconda3/envs/python3_10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold=4, result = TrainOutput(global_step=4226, training_loss=0.0025673676080894925, metrics={'train_runtime': 3361.8695, 'train_samples_per_second': 10.059, 'train_steps_per_second': 1.257, 'total_flos': 8.252965690146816e+16, 'train_loss': 0.0025673676080894925, 'epoch': 2.0})\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "last_checkpoint_path = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_0/checkpoint-4226'\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(np.arange(len(datasets)))):\n",
    "    if fold < 1:\n",
    "        continue\n",
    "    print(f\"fold={fold} start, train_index={train_index}, val_index={val_index}\")\n",
    "    train_dataset = datasets.select(train_index)\n",
    "    eval_dataset = datasets.select(val_index)\n",
    "    print(f\"train data: {len(train_dataset)}, eval: {len(eval_dataset)}\")\n",
    "\n",
    "    output_path = f'/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_{fold}'\n",
    "    train_args, lora_config = build_arguments(output_path)\n",
    "    if last_checkpoint_path:\n",
    "        model = load_model_v2(model_path, last_checkpoint_path, device)\n",
    "    else:\n",
    "        model = load_model(model_path, device)\n",
    "        model = get_peft_model(model, load_config)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "    trainer = build_trainer_v2(model, tokenizer, train_args, train_dataset, eval_dataset)\n",
    "    train_result = trainer.train()\n",
    "    print(f\"fold={fold}, result = {train_result}\")\n",
    "    results.append(train_result)\n",
    "\n",
    "    # 清理旧模型和优化器以释放显存\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    last_checkpoint_path = find_last_checkpoint(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23425a-f83e-4357-99e1-25d6ae5eb301",
   "metadata": {},
   "source": [
    "收集5轮训练的数据。\n",
    "\n",
    "第0轮训练数据：\n",
    "| Epoch |\tTraining Loss\t| Validation Loss |\n",
    "| --- | --- | --- |\n",
    "|1\t| 0.0233 |\t0.02189 | \n",
    "|2\t| 0.0138 | 0.01614 |\n",
    "|3\t| 0.008800 |\t0.011420 |\n",
    "|4\t| 0.004600 |\t0.013666 |\n",
    "|5\t| 0.003200 |\t0.004718 |\n",
    "|6\t| 0.003000 |\t0.004082 |\n",
    "|7\t| 0.007200 |\t0.001999 |\n",
    "|8\t| 0.000000 |\t0.000814 |\n",
    "|9\t| 0.004900 | 0.002273 |\n",
    "|10\t| 0.010200 | 0.002139 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22861379-fa9d-452b-b158-58ea6a2967f1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "对比前面[欺诈文本分类微调（七）—— lora单卡二次调优](https://golfxiao.blog.csdn.net/article/details/141500352)训练进行到2300步左右（大概两遍数据）就开始过拟合，主要现象是验证损失到0.0161就不再下降反而开始升高，K折交叉训练直到第4次迭代（大概八遍数据）过后才达到损失最低点，第5次迭代才出现了略微的过拟合（相比于第4次），过拟合的现象得到了极大的缓解，验证损失也降到了一个更低的值0.000814，这说明数据在训练和验证中被充分的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0332759a-cf6a-45a0-89b1-402259f49f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:20<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1151, fp:14, fn:74, tp:1109\n",
      "precision: 0.98753339269813, recall: 0.937447168216399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "checkpoint_path='/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_0/checkpoint-4226'\n",
    "evaluate(model_path, checkpoint_path, evaldata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0c99b9e-5fbc-40a9-b890-476c41650934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:20<00:00, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1158, fp:7, fn:16, tp:1167\n",
      "precision: 0.9940374787052811, recall: 0.9864750633981403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "checkpoint_path='/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_1/checkpoint-4226'\n",
    "evaluate(model_path, checkpoint_path, evaldata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "231ff930-2b69-40fa-9df6-97475448f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:20<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1162, fp:3, fn:3, tp:1180\n",
      "precision: 0.9974640743871513, recall: 0.9974640743871513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "checkpoint_path='/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_2/checkpoint-4226'\n",
    "evaluate(model_path, checkpoint_path, evaldata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "923ed037-c122-48d7-8109-bf0c488ae302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:20<00:00, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1164, fp:1, fn:4, tp:1179\n",
      "precision: 0.9991525423728813, recall: 0.9966187658495351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "checkpoint_path='/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_3/checkpoint-4226'\n",
    "evaluate(model_path, checkpoint_path, evaldata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cd8952a-47a5-4a36-9e20-99657bbc7434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:22<00:00, 11.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1164, fp:1, fn:1, tp:1182\n",
      "precision: 0.9991546914623838, recall: 0.9991546914623838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "checkpoint_path='/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_4/checkpoint-4226'\n",
    "evaluate(model_path, checkpoint_path, evaldata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44a9b6-a619-4212-93f0-ad18e7fb3da9",
   "metadata": {},
   "source": [
    "## 评估测试\n",
    "由于交叉训练中验证集和训练集都参与了模型学习参数的更新，所以用验证集进行评估已经没有意义。我们直接用测试集进行最后的评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbe40c98-e0c1-483c-8386-5a461aad7d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2349/2349 [03:19<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1135, fp:32, fn:128, tp:1054\n",
      "precision: 0.9705340699815838, recall: 0.8917089678510999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "checkpoint_path='/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_0/checkpoint-4226'\n",
    "testdata_path = '/data2/anti_fraud/dataset/test0819.jsonl'\n",
    "evaluate(model_path, checkpoint_path, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5923d5e8-d01b-463d-9e22-9a9eff562fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2349/2349 [03:21<00:00, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1133, fp:34, fn:64, tp:1118\n",
      "precision: 0.9704861111111112, recall: 0.9458544839255499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "checkpoint_path='/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_2/checkpoint-4226'\n",
    "testdata_path = '/data2/anti_fraud/dataset/test0819.jsonl'\n",
    "evaluate(model_path, checkpoint_path, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "573fe3be-5403-4a32-8874-3908094b2eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2349/2349 [03:21<00:00, 11.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1128, fp:39, fn:64, tp:1118\n",
      "precision: 0.9662921348314607, recall: 0.9458544839255499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "checkpoint_path='/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_3/checkpoint-4226'\n",
    "testdata_path = '/data2/anti_fraud/dataset/test0819.jsonl'\n",
    "evaluate(model_path, checkpoint_path, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdc0b85e-d59c-41ca-a011-958f656258ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2349/2349 [03:22<00:00, 11.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1124, fp:43, fn:50, tp:1132\n",
      "precision: 0.963404255319149, recall: 0.9576988155668359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "checkpoint_path='/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0903_4/checkpoint-4226'\n",
    "testdata_path = '/data2/anti_fraud/dataset/test0819.jsonl'\n",
    "evaluate(model_path, checkpoint_path, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7fc12-0d65-42fd-987d-41762f51a20c",
   "metadata": {},
   "source": [
    "**小结**：本文通过引入K折交叉验证方法，循环选择不同的训练集和验证集进行多次迭代训练，将损失降到了一个更低的值，也在很大程度上缓解了[前面每次训练]过程中都出现的过拟合现象。最终在从未见过的测试数据集上进行评测时，精确率和召回率指标也有了一个大的提升，K折交叉验证这种方法确实能让模型对数据学习的更充分，最终得到的模型泛化能力也更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e90eb-495c-49fd-b1ab-d4372b3fa485",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "- [交叉验证方法汇总](https://blog.csdn.net/WHYbeHERE/article/details/108192957)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
