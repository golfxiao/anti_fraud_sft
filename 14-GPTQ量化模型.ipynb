{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f0d8f8-40dd-4c18-b48b-930b1b9408af",
   "metadata": {},
   "source": [
    "## 引言\n",
    "\n",
    "量化的本质：通过将模型参数从高精度（例如32位）降低到低精度（例如8位），来缩小模型体积。\n",
    "\n",
    "本文将采用一种训练后量化方法GPTQ，对前文已经训练并合并过的模型文件进行量化，通过比较模型量化前后的评测指标，来测试量化对模型性能的影响。\n",
    "\n",
    "GPTQ的核心思想在于：将所有权重压缩到8位或4位量化中，通过最小化与该权重的均方误差来实现。在推理过程中，它将动态地将权重解量化为float16，以提高性能，同时保持较低的内存占用率。\n",
    "\n",
    "> 注：均方误差是评估两个数值数据集之间差异的一种常用方法，它通过计算量化后权重与原始权重之间的均方误差，并使之最小化，来减少量化过程中引入的误差，以保持模型在推理时的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2fdaa-174b-43d6-a42f-1d60c25a8284",
   "metadata": {},
   "source": [
    "## 8位量化\n",
    "\n",
    "#### 加载量化模型\n",
    "\n",
    "首先引入必要的包，其中：\n",
    "- auto_gptq: 一个用于模型量化的库，通常用于减少模型的内存占用和计算消耗。\n",
    "- AutoGPTQForCausalLM: 用于加载和使用经过量化的因果语言模型。\n",
    "- BaseQuantizeConfig: 定义量化模型时所需的参数，例如量化精度。\n",
    "- AutoTokenizer：transformers库提供的分词器，用于处理文本分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0391b2f7-812c-4af0-a258-ca7aa2cef238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df687191-b05f-4bcb-a3b9-9c8343fa1ea0",
   "metadata": {},
   "source": [
    "定义量化任务要使用的设备，并指定模型的原始路径`model_path`和量化后的路径`quant_path`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94bccb14-0c3a-4d12-a879-66d8352e62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = 'cuda'\n",
    "model_path = \"/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud_1__0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846c8cf0-f5f4-485d-9259-c40435f4f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "配置量化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57c61af-0df9-4d5c-8f4f-67814e163342",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=8, \n",
    "    group_size=128,   # 分组量化\n",
    "    damp_percent=0.01,\n",
    "    desc_act=False,  \n",
    "    static_groups=False,\n",
    "    sym=True,\n",
    "    true_sequential=True,\n",
    "    model_name_or_path=None,\n",
    "    model_file_base_name=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34c36c-3e40-4114-ae0c-6c83e1bbef1f",
   "metadata": {},
   "source": [
    "- group_size：量化时的分组大小，分组量化可以提高计算效率，通常设置为 128 是一个合理的选择，适合大多数模型。\n",
    "- damp_percent：控制量化过程中对权重的平滑处理，防止过度量化导致的性能下降。默认值 0.01 通常是一个良好的起点，如果量化不佳，可以增加此值。\n",
    "- desc_act：控制是否使用描述性激活，设置为 False 可以加速推理，如果模型的精度更重要，可以设置为 True。\n",
    "- static_groups： 是否使用静态分组。静态分组可以提高推理效率， 如果模型结构固定且不需要动态调整，可以设置为 True。否则，保持为 False 以支持动态分组。\n",
    "- sym： 指定是否使用对称量化。对称量化可以简化计算，如果模型对称性较好，可以设置为 True。\n",
    "- true_sequential： 控制是否使用真实的顺序量化。真实顺序量化可以提高模型的表现，但可能会增加计算复杂性。如果模型对顺序敏感，可以设置为 True。\n",
    "- model_file_base_name：指定生成的量化模型文件名称，最终体现在输出文件的命名上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dbdaf9-b367-4800-a3df-6f3557aed881",
   "metadata": {},
   "source": [
    "加载分词器，并根据配置`quantize_config`指定的量化位数来加载模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b47e4e-1a27-48d9-8ee2-04621333ecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e558a18a61d4449d9e029a8d08ca76ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c706385-0cfd-441b-bb09-c2598874bacc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 校准量化参数\n",
    "\n",
    "GPTQ采用权重分组量化（如上面的配置中128列为一组），一个分组内的参数采用逐个量化，在每个参数被量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。\n",
    "\n",
    "![分组量化示意](./img/量化模型/group_quant.png)\n",
    "因此，GPTQ 量化需要准备校准数据集，我们这里采用一个以前生成的测试数据集作为校准数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ce4b45-7d35-48b1-a1b3-77c3bd79c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    conversations = []\n",
    "    with open(path, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "        conversations = [dialog['messages'] for dialog in data]\n",
    "        return conversations\n",
    "\n",
    "eval_data_path = '/data2/anti_fraud/dataset/test_chatml_0815.jsonl'\n",
    "conversations = load_jsonl(eval_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6016f-a3b3-406d-b5bd-ef51a4613028",
   "metadata": {},
   "outputs": [],
   "source": [
    "数据格式是一个标准的聊天模板，示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "097c5a55-6e40-4e8d-95bb-76551a41f53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': '\\n下面是一段对话文本, 请分析对话内容是否有诈骗风险，以json格式输出你的判断结果(is_fraud: true/false)。\\n\\n\\n发言人3: 那就是说看上半年我们的三四月份会不会有一些这个相关的一些这个缓解，就是说这方面的一些矛盾的一些缓解，债务的一个情况的一些缓解，那我们还要继续观察。\\n发言人2: 好的，蒋总，那我们看一下那个其他投资者有没有什么其他问题。\\n发言人1: 大家好，通过网络端接入的投资者可点击举手连麦等候提问，或在文字交流区提交您的问题，通过电话端接入的投资者请按星一键提问。先按星号键，再按一键，谢谢。大家好，通过网络端接入的投资者可点击举手连麦，然后提问，或在文字交流区提交您的问题。通过电话端接入的投资者请按星一键提问。\\n发言人1: 先按星号键，再按数字一键，谢谢。'},\n",
       " {'role': 'assistant', 'content': '{\"is_fraud\": false}'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3694c-6182-4e17-ba96-719e20e3dd11",
   "metadata": {},
   "source": [
    "定义一个预处理函数，将文本数据预处理为张量数据。\n",
    "- tokenizer.apply_chat_template：将消息格式化为Qwen模型需要的提示词格式。\n",
    "- tokenizer([text])：使用tokenizer对文本进行分词，并将token转换为ID值。\n",
    "- torch.tensor：将token_id转换为tensor张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d748c2c-779d-4b0a-8fbb-5259afbfd4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset, max_len=1024):\n",
    "    data = []\n",
    "    for msg in dataset:\n",
    "        text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\n",
    "        model_inputs = tokenizer([text])\n",
    "        input_ids = torch.tensor(model_inputs.input_ids[:max_len], dtype=torch.int)\n",
    "        data.append(dict(input_ids=input_ids, attention_mask=input_ids.ne(tokenizer.pad_token_id)))\n",
    "    return data\n",
    "\n",
    "dataset = preprocess(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca4601-e2e2-4635-96c2-b133d590d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "配置日志显示格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69a1c3-10db-4fdb-8a62-b5f9e6c41c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d57daa2-5409-4476-937c-b307b066978b",
   "metadata": {},
   "source": [
    "开始量化，使用校准数据集来动态调整量化参数，使模型在量化时学习并适应数据分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e2e66a2-51b0-4030-9c4b-2f1cfa6a3467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/28...\n",
      "INFO - Start quantizing layer 2/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/28...\n",
      "INFO - Start quantizing layer 3/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/28...\n",
      "INFO - Start quantizing layer 4/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/28...\n",
      "INFO - Start quantizing layer 5/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/28...\n",
      "INFO - Start quantizing layer 6/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/28...\n",
      "INFO - Start quantizing layer 7/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/28...\n",
      "INFO - Start quantizing layer 8/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/28...\n",
      "INFO - Start quantizing layer 9/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/28...\n",
      "INFO - Start quantizing layer 10/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/28...\n",
      "INFO - Start quantizing layer 11/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/28...\n",
      "INFO - Start quantizing layer 12/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/28...\n",
      "INFO - Start quantizing layer 13/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/28...\n",
      "INFO - Start quantizing layer 14/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/28...\n",
      "INFO - Start quantizing layer 15/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/28...\n",
      "INFO - Start quantizing layer 16/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/28...\n",
      "INFO - Start quantizing layer 17/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/28...\n",
      "INFO - Start quantizing layer 18/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/28...\n",
      "INFO - Start quantizing layer 19/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/28...\n",
      "INFO - Start quantizing layer 20/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/28...\n",
      "INFO - Start quantizing layer 21/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/28...\n",
      "INFO - Start quantizing layer 22/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/28...\n",
      "INFO - Start quantizing layer 23/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/28...\n",
      "INFO - Start quantizing layer 24/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/28...\n",
      "INFO - Start quantizing layer 25/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/28...\n",
      "INFO - Start quantizing layer 26/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/28...\n",
      "INFO - Start quantizing layer 27/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/28...\n",
      "INFO - Start quantizing layer 28/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/28...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30min 52s, sys: 3min 40s, total: 34min 32s\n",
      "Wall time: 27min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.quantize(dataset, cache_examples_on_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78655830-83d5-43e0-9636-f690ec8ec026",
   "metadata": {},
   "source": [
    "保存量化后的模型和分词器状态。\n",
    "> use_safetensors=True 参数表示使用安全张量格式（SafeTensors）进行保存，具有更好的安全性和性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30446a9e-1bdc-4aff-b60f-e159930174b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int8/tokenizer_config.json',\n",
       " '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int8/special_tokens_map.json',\n",
       " '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int8/vocab.json',\n",
       " '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int8/merges.txt',\n",
       " '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int8/added_tokens.json',\n",
       " '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int8/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_path = \"/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int8\"\n",
    "model.save_quantized(quant_path, use_safetensors=True)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911debe9-ec9c-40af-9741-e01658c162c6",
   "metadata": {},
   "source": [
    "## 4位量化\n",
    "作为对比，我们也进行一个4位量化，与8位量化的区别只在于量化配置时的参数bits改成了4，其它都不作改变."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0265cbf-b488-49e8-be71-1cf2974bc6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_config_int4 = BaseQuantizeConfig(\n",
    "    bits=4,           # 4位量化\n",
    "    group_size=128,   # 分组量化\n",
    "    damp_percent=0.01,\n",
    "    desc_act=False,  \n",
    "    static_groups=False,\n",
    "    sym=True,\n",
    "    true_sequential=True,\n",
    "    model_name_or_path=None,\n",
    "    model_file_base_name=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "275ca132-9708-41c3-b614-f8773f483fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c6558981de4a59b7777a36fe93d03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_int4 = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config_int4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ed44c60-4621-4cf6-a42f-051d4b80cbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/28...\n",
      "INFO - Start quantizing layer 2/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/28...\n",
      "INFO - Start quantizing layer 3/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/28...\n",
      "INFO - Start quantizing layer 4/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/28...\n",
      "INFO - Start quantizing layer 5/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/28...\n",
      "INFO - Start quantizing layer 6/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/28...\n",
      "INFO - Start quantizing layer 7/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/28...\n",
      "INFO - Start quantizing layer 8/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/28...\n",
      "INFO - Start quantizing layer 9/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/28...\n",
      "INFO - Start quantizing layer 10/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/28...\n",
      "INFO - Start quantizing layer 11/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/28...\n",
      "INFO - Start quantizing layer 12/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/28...\n",
      "INFO - Start quantizing layer 13/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/28...\n",
      "INFO - Start quantizing layer 14/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/28...\n",
      "INFO - Start quantizing layer 15/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/28...\n",
      "INFO - Start quantizing layer 16/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/28...\n",
      "INFO - Start quantizing layer 17/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/28...\n",
      "INFO - Start quantizing layer 18/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/28...\n",
      "INFO - Start quantizing layer 19/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/28...\n",
      "INFO - Start quantizing layer 20/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/28...\n",
      "INFO - Start quantizing layer 21/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/28...\n",
      "INFO - Start quantizing layer 22/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/28...\n",
      "INFO - Start quantizing layer 23/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/28...\n",
      "INFO - Start quantizing layer 24/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/28...\n",
      "INFO - Start quantizing layer 25/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/28...\n",
      "INFO - Start quantizing layer 26/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/28...\n",
      "INFO - Start quantizing layer 27/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/28...\n",
      "INFO - Start quantizing layer 28/28\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/28...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/28...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/28...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37min 11s, sys: 3min 2s, total: 40min 13s\n",
      "Wall time: 31min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_int4.quantize(dataset, cache_examples_on_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "029b649b-591b-4dec-82e1-cd8f0a0fbf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int4/tokenizer_config.json',\n",
       " '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int4/special_tokens_map.json',\n",
       " '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int4/vocab.json',\n",
       " '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int4/merges.txt',\n",
       " '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int4/added_tokens.json',\n",
       " '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int4/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_int4_path = \"/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int4\"\n",
    "model_int4.save_quantized(quant_int4_path, use_safetensors=True)\n",
    "tokenizer.save_pretrained(quant_int4_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc44549f-6dae-47a3-9f9c-0db728d55f24",
   "metadata": {},
   "source": [
    "## 评测\n",
    "与[前文](https://golfxiao.blog.csdn.net/article/details/141569237)不同，这里统一采用测试数据集进行评测，以评估模型的最终性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf6284-442c-4013-9a41-47e8740aec62",
   "metadata": {},
   "source": [
    "#### 原始模型16位评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb06dc5b-17fa-42ef-8144-7e2b9e9513a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234633a5f2174def9b614357f69ffbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2349/2349 [01:52<00:00, 20.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1136, fp:31, fn:162, tp:1020\n",
      "precision: 0.9705042816365367, recall: 0.8629441624365483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "testdata_path = '/data2/anti_fraud/dataset/test0819.jsonl'\n",
    "evaluate(model_path, '', testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e8036e-9633-4251-a2f1-874f5363802c",
   "metadata": {},
   "source": [
    "#### 量化8位模型评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415f377-37e7-4b0c-9b27-e42c940e6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run evaluate.py\n",
    "testdata_path = '/data2/anti_fraud/dataset/test0819.jsonl'\n",
    "model_int8_path = '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int8'\n",
    "evaluate(model_gptq_path, '', testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18592b1-b952-4ba1-801e-5d1dc3d4736e",
   "metadata": {},
   "source": [
    "tn：1134, fp:33, fn:158, tp:1024\n",
    "\n",
    "precision: 0.9687795648060549, recall: 0.8663282571912013"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e735ac-202e-42ed-bbf6-f4bf60369f73",
   "metadata": {},
   "source": [
    "#### 量化4位模型评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87bc6fe5-617e-4686-b45a-1bd6f171bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int4 were not used when initializing Qwen2ForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.o_proj.bias']\n",
      "- This IS expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "progress:   0%|          | 0/2349 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   0%|          | 8/2349 [00:00<04:13,  9.23it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   1%|          | 16/2349 [00:01<03:51, 10.08it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   1%|          | 24/2349 [00:02<03:40, 10.54it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   1%|▏         | 32/2349 [00:03<03:40, 10.48it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   2%|▏         | 40/2349 [00:03<03:36, 10.67it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   2%|▏         | 48/2349 [00:04<03:30, 10.94it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   2%|▏         | 56/2349 [00:05<03:29, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   3%|▎         | 64/2349 [00:06<03:49,  9.96it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   3%|▎         | 72/2349 [00:07<04:01,  9.42it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   3%|▎         | 80/2349 [00:07<03:53,  9.74it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   4%|▎         | 88/2349 [00:08<03:44, 10.05it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   4%|▍         | 96/2349 [00:09<03:52,  9.70it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   4%|▍         | 104/2349 [00:10<03:47,  9.88it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   5%|▍         | 112/2349 [00:11<03:42, 10.04it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   5%|▌         | 120/2349 [00:11<03:33, 10.45it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   5%|▌         | 128/2349 [00:12<03:32, 10.43it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   6%|▌         | 136/2349 [00:13<03:25, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   6%|▌         | 144/2349 [00:13<03:20, 11.00it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   6%|▋         | 152/2349 [00:14<03:19, 11.02it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   7%|▋         | 160/2349 [00:15<03:19, 10.97it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   7%|▋         | 168/2349 [00:16<03:38,  9.98it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   7%|▋         | 176/2349 [00:17<03:44,  9.67it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   8%|▊         | 184/2349 [00:18<03:41,  9.76it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   8%|▊         | 192/2349 [00:18<03:36,  9.99it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   9%|▊         | 200/2349 [00:19<03:32, 10.11it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   9%|▉         | 208/2349 [00:20<03:40,  9.73it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:   9%|▉         | 216/2349 [00:21<03:33, 10.01it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  10%|▉         | 224/2349 [00:21<03:24, 10.41it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  10%|▉         | 232/2349 [00:22<03:23, 10.41it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  10%|█         | 240/2349 [00:23<03:21, 10.45it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  11%|█         | 248/2349 [00:24<03:14, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  11%|█         | 256/2349 [00:24<03:14, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  11%|█         | 264/2349 [00:25<03:11, 10.91it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  12%|█▏        | 272/2349 [00:26<03:09, 10.95it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  12%|█▏        | 280/2349 [00:27<03:11, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  12%|█▏        | 288/2349 [00:27<03:12, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  13%|█▎        | 296/2349 [00:28<03:28,  9.86it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  13%|█▎        | 304/2349 [00:29<03:35,  9.48it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  13%|█▎        | 312/2349 [00:30<03:38,  9.33it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  14%|█▎        | 320/2349 [00:31<03:30,  9.62it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  14%|█▍        | 328/2349 [00:32<03:21, 10.02it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  14%|█▍        | 336/2349 [00:32<03:16, 10.26it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  15%|█▍        | 344/2349 [00:33<03:13, 10.36it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  15%|█▍        | 352/2349 [00:34<03:11, 10.40it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  15%|█▌        | 360/2349 [00:35<03:06, 10.69it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  16%|█▌        | 368/2349 [00:35<03:06, 10.61it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  16%|█▌        | 376/2349 [00:36<03:01, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  16%|█▋        | 384/2349 [00:37<03:03, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  17%|█▋        | 392/2349 [00:38<03:04, 10.63it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  17%|█▋        | 400/2349 [00:38<03:05, 10.53it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  17%|█▋        | 408/2349 [00:39<03:01, 10.67it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  18%|█▊        | 416/2349 [00:40<02:57, 10.91it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  18%|█▊        | 424/2349 [00:41<03:00, 10.65it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  18%|█▊        | 432/2349 [00:41<02:53, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  19%|█▊        | 440/2349 [00:42<02:51, 11.14it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  19%|█▉        | 448/2349 [00:43<02:45, 11.49it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  19%|█▉        | 456/2349 [00:43<02:49, 11.14it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  20%|█▉        | 464/2349 [00:44<02:50, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  20%|██        | 472/2349 [00:45<02:53, 10.80it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  20%|██        | 480/2349 [00:46<02:55, 10.68it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  21%|██        | 488/2349 [00:46<02:54, 10.64it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  21%|██        | 496/2349 [00:47<02:55, 10.58it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  21%|██▏       | 504/2349 [00:48<02:48, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  22%|██▏       | 512/2349 [00:49<02:47, 10.99it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  22%|██▏       | 520/2349 [00:49<02:41, 11.31it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  22%|██▏       | 528/2349 [00:50<02:44, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  23%|██▎       | 536/2349 [00:51<02:46, 10.91it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  23%|██▎       | 544/2349 [00:51<02:47, 10.80it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  23%|██▎       | 552/2349 [00:52<02:47, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  24%|██▍       | 560/2349 [00:53<02:48, 10.63it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  24%|██▍       | 568/2349 [00:54<02:59,  9.94it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  25%|██▍       | 576/2349 [00:55<03:06,  9.53it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  25%|██▍       | 584/2349 [00:56<03:00,  9.76it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  25%|██▌       | 592/2349 [00:56<02:57,  9.88it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  26%|██▌       | 600/2349 [00:57<02:49, 10.29it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  26%|██▌       | 608/2349 [00:58<02:48, 10.35it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  26%|██▌       | 616/2349 [00:59<02:39, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  27%|██▋       | 624/2349 [00:59<02:35, 11.09it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  27%|██▋       | 632/2349 [01:00<02:33, 11.19it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  27%|██▋       | 640/2349 [01:01<02:31, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  28%|██▊       | 648/2349 [01:01<02:34, 11.01it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  28%|██▊       | 656/2349 [01:02<02:35, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  28%|██▊       | 664/2349 [01:03<02:36, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  29%|██▊       | 672/2349 [01:04<02:37, 10.64it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  29%|██▉       | 680/2349 [01:04<02:36, 10.65it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  29%|██▉       | 688/2349 [01:05<02:37, 10.58it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  30%|██▉       | 696/2349 [01:06<02:34, 10.69it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  30%|██▉       | 704/2349 [01:07<02:32, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  30%|███       | 712/2349 [01:07<02:29, 10.95it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  31%|███       | 720/2349 [01:08<02:25, 11.20it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  31%|███       | 728/2349 [01:09<02:24, 11.21it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  31%|███▏      | 736/2349 [01:09<02:24, 11.14it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  32%|███▏      | 744/2349 [01:10<02:27, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  32%|███▏      | 752/2349 [01:11<02:25, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  32%|███▏      | 760/2349 [01:12<02:23, 11.04it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  33%|███▎      | 768/2349 [01:12<02:25, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  33%|███▎      | 776/2349 [01:13<02:25, 10.81it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  33%|███▎      | 784/2349 [01:14<02:22, 10.97it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  34%|███▎      | 792/2349 [01:15<02:25, 10.68it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  34%|███▍      | 800/2349 [01:15<02:21, 10.92it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  34%|███▍      | 808/2349 [01:16<02:19, 11.06it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  35%|███▍      | 816/2349 [01:17<02:17, 11.11it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  35%|███▌      | 824/2349 [01:18<02:20, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  35%|███▌      | 832/2349 [01:18<02:31, 10.02it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  36%|███▌      | 840/2349 [01:19<02:28, 10.13it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  36%|███▌      | 848/2349 [01:20<02:26, 10.24it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  36%|███▋      | 856/2349 [01:21<02:25, 10.27it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  37%|███▋      | 864/2349 [01:21<02:20, 10.61it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  37%|███▋      | 872/2349 [01:22<02:18, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  37%|███▋      | 880/2349 [01:23<02:14, 10.95it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  38%|███▊      | 888/2349 [01:24<02:13, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  38%|███▊      | 896/2349 [01:24<02:09, 11.22it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  38%|███▊      | 904/2349 [01:25<02:11, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  39%|███▉      | 912/2349 [01:26<02:12, 10.87it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  39%|███▉      | 920/2349 [01:27<02:13, 10.66it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  40%|███▉      | 928/2349 [01:27<02:11, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  40%|███▉      | 936/2349 [01:28<02:08, 11.02it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  40%|████      | 944/2349 [01:29<02:09, 10.87it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  41%|████      | 952/2349 [01:30<02:10, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  41%|████      | 960/2349 [01:30<02:07, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  41%|████      | 968/2349 [01:31<02:08, 10.72it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  42%|████▏     | 976/2349 [01:32<02:05, 10.91it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  42%|████▏     | 984/2349 [01:32<02:06, 10.82it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  42%|████▏     | 992/2349 [01:33<02:01, 11.19it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  43%|████▎     | 1000/2349 [01:34<01:59, 11.28it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  43%|████▎     | 1008/2349 [01:35<01:57, 11.46it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  43%|████▎     | 1016/2349 [01:35<02:04, 10.72it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  44%|████▎     | 1024/2349 [01:36<02:00, 10.99it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  44%|████▍     | 1032/2349 [01:37<02:01, 10.82it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  44%|████▍     | 1040/2349 [01:38<01:59, 10.96it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  45%|████▍     | 1048/2349 [01:38<02:00, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  45%|████▍     | 1056/2349 [01:39<01:59, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  45%|████▌     | 1064/2349 [01:40<01:57, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  46%|████▌     | 1072/2349 [01:41<01:58, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  46%|████▌     | 1080/2349 [01:41<01:58, 10.69it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  46%|████▋     | 1088/2349 [01:42<01:59, 10.53it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  47%|████▋     | 1096/2349 [01:43<01:56, 10.73it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  47%|████▋     | 1104/2349 [01:43<01:54, 10.84it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  47%|████▋     | 1112/2349 [01:44<01:50, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  48%|████▊     | 1120/2349 [01:45<01:50, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  48%|████▊     | 1128/2349 [01:46<01:52, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  48%|████▊     | 1136/2349 [01:46<01:52, 10.80it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  49%|████▊     | 1144/2349 [01:47<01:50, 10.94it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  49%|████▉     | 1152/2349 [01:48<01:48, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  49%|████▉     | 1160/2349 [01:49<01:46, 11.21it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  50%|████▉     | 1168/2349 [01:49<01:48, 10.91it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  50%|█████     | 1176/2349 [01:50<01:44, 11.17it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  50%|█████     | 1184/2349 [01:51<01:53, 10.26it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  51%|█████     | 1192/2349 [01:52<01:55, 10.02it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  51%|█████     | 1200/2349 [01:53<01:58,  9.68it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  51%|█████▏    | 1208/2349 [01:53<01:54,  9.95it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  52%|█████▏    | 1216/2349 [01:54<01:56,  9.69it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  52%|█████▏    | 1224/2349 [01:55<01:50, 10.14it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  52%|█████▏    | 1232/2349 [01:56<01:46, 10.51it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  53%|█████▎    | 1240/2349 [01:56<01:45, 10.48it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  53%|█████▎    | 1248/2349 [01:57<01:55,  9.56it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  53%|█████▎    | 1256/2349 [01:58<01:47, 10.13it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  54%|█████▍    | 1264/2349 [01:59<01:46, 10.23it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  54%|█████▍    | 1272/2349 [02:00<01:44, 10.33it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  54%|█████▍    | 1280/2349 [02:00<01:41, 10.52it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  55%|█████▍    | 1288/2349 [02:01<01:38, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  55%|█████▌    | 1296/2349 [02:02<01:35, 11.00it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  56%|█████▌    | 1304/2349 [02:03<01:35, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  56%|█████▌    | 1312/2349 [02:03<01:33, 11.08it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  56%|█████▌    | 1320/2349 [02:04<01:34, 10.84it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  57%|█████▋    | 1328/2349 [02:05<01:34, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  57%|█████▋    | 1336/2349 [02:05<01:35, 10.66it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  57%|█████▋    | 1344/2349 [02:06<01:32, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  58%|█████▊    | 1352/2349 [02:07<01:33, 10.72it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  58%|█████▊    | 1360/2349 [02:08<01:32, 10.67it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  58%|█████▊    | 1368/2349 [02:08<01:30, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  59%|█████▊    | 1376/2349 [02:09<01:30, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  59%|█████▉    | 1384/2349 [02:10<01:27, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  59%|█████▉    | 1392/2349 [02:11<01:28, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  60%|█████▉    | 1400/2349 [02:12<01:32, 10.31it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  60%|█████▉    | 1408/2349 [02:12<01:28, 10.61it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  60%|██████    | 1416/2349 [02:13<01:28, 10.56it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  61%|██████    | 1424/2349 [02:14<01:33,  9.94it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  61%|██████    | 1432/2349 [02:15<01:30, 10.17it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  61%|██████▏   | 1440/2349 [02:15<01:28, 10.28it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  62%|██████▏   | 1448/2349 [02:16<01:24, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  62%|██████▏   | 1456/2349 [02:17<01:23, 10.76it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  62%|██████▏   | 1464/2349 [02:18<01:22, 10.74it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  63%|██████▎   | 1472/2349 [02:18<01:27, 10.04it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  63%|██████▎   | 1480/2349 [02:19<01:27,  9.91it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  63%|██████▎   | 1488/2349 [02:20<01:30,  9.55it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  64%|██████▎   | 1496/2349 [02:21<01:33,  9.16it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  64%|██████▍   | 1504/2349 [02:22<01:32,  9.12it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  64%|██████▍   | 1512/2349 [02:23<01:32,  9.09it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  65%|██████▍   | 1520/2349 [02:24<01:25,  9.64it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  65%|██████▌   | 1528/2349 [02:24<01:21, 10.10it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  65%|██████▌   | 1536/2349 [02:25<01:19, 10.22it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  66%|██████▌   | 1544/2349 [02:26<01:17, 10.41it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  66%|██████▌   | 1552/2349 [02:27<01:16, 10.48it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  66%|██████▋   | 1560/2349 [02:27<01:15, 10.47it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  67%|██████▋   | 1568/2349 [02:28<01:12, 10.71it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  67%|██████▋   | 1576/2349 [02:29<01:10, 10.90it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  67%|██████▋   | 1584/2349 [02:29<01:09, 11.03it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  68%|██████▊   | 1592/2349 [02:30<01:09, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  68%|██████▊   | 1600/2349 [02:31<01:09, 10.70it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  68%|██████▊   | 1608/2349 [02:32<01:08, 10.87it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  69%|██████▉   | 1616/2349 [02:32<01:06, 10.95it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  69%|██████▉   | 1624/2349 [02:33<01:07, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  69%|██████▉   | 1632/2349 [02:34<01:06, 10.73it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  70%|██████▉   | 1640/2349 [02:35<01:04, 11.00it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  70%|███████   | 1648/2349 [02:35<01:04, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  70%|███████   | 1656/2349 [02:36<01:02, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  71%|███████   | 1664/2349 [02:37<01:01, 11.16it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  71%|███████   | 1672/2349 [02:38<01:01, 11.02it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  72%|███████▏  | 1680/2349 [02:38<01:02, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  72%|███████▏  | 1688/2349 [02:39<01:00, 10.97it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  72%|███████▏  | 1696/2349 [02:40<00:58, 11.07it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  73%|███████▎  | 1704/2349 [02:41<00:59, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  73%|███████▎  | 1712/2349 [02:41<00:59, 10.75it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  73%|███████▎  | 1720/2349 [02:42<00:56, 11.18it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  74%|███████▎  | 1728/2349 [02:43<00:55, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  74%|███████▍  | 1736/2349 [02:43<00:53, 11.36it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  74%|███████▍  | 1744/2349 [02:44<00:53, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  75%|███████▍  | 1752/2349 [02:45<00:53, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  75%|███████▍  | 1760/2349 [02:46<00:58, 10.14it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  75%|███████▌  | 1768/2349 [02:47<00:59,  9.78it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  76%|███████▌  | 1776/2349 [02:48<01:01,  9.37it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  76%|███████▌  | 1784/2349 [02:48<01:00,  9.28it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  76%|███████▋  | 1792/2349 [02:49<00:57,  9.74it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  77%|███████▋  | 1800/2349 [02:50<00:54, 10.15it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  77%|███████▋  | 1808/2349 [02:51<00:57,  9.46it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  77%|███████▋  | 1816/2349 [02:52<00:58,  9.12it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  78%|███████▊  | 1824/2349 [02:53<00:54,  9.67it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  78%|███████▊  | 1832/2349 [02:53<00:51, 10.09it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  78%|███████▊  | 1840/2349 [02:54<00:49, 10.18it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  79%|███████▊  | 1848/2349 [02:55<00:47, 10.64it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  79%|███████▉  | 1856/2349 [02:55<00:46, 10.61it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  79%|███████▉  | 1864/2349 [02:56<00:44, 10.80it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  80%|███████▉  | 1872/2349 [02:57<00:43, 10.92it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  80%|████████  | 1880/2349 [02:58<00:41, 11.25it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  80%|████████  | 1888/2349 [02:58<00:40, 11.33it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  81%|████████  | 1896/2349 [02:59<00:40, 11.21it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  81%|████████  | 1904/2349 [03:00<00:40, 11.02it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  81%|████████▏ | 1912/2349 [03:00<00:38, 11.26it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  82%|████████▏ | 1920/2349 [03:01<00:37, 11.45it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  82%|████████▏ | 1928/2349 [03:02<00:37, 11.15it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  82%|████████▏ | 1936/2349 [03:03<00:37, 10.97it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  83%|████████▎ | 1944/2349 [03:03<00:36, 11.13it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  83%|████████▎ | 1952/2349 [03:04<00:36, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  83%|████████▎ | 1960/2349 [03:05<00:38, 10.10it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  84%|████████▍ | 1968/2349 [03:06<00:39,  9.73it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  84%|████████▍ | 1976/2349 [03:07<00:37,  9.86it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  84%|████████▍ | 1984/2349 [03:07<00:36,  9.95it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  85%|████████▍ | 1992/2349 [03:08<00:35, 10.13it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  85%|████████▌ | 2000/2349 [03:09<00:33, 10.37it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  85%|████████▌ | 2008/2349 [03:10<00:32, 10.39it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  86%|████████▌ | 2016/2349 [03:10<00:31, 10.62it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  86%|████████▌ | 2024/2349 [03:11<00:30, 10.55it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  87%|████████▋ | 2032/2349 [03:12<00:28, 11.03it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  87%|████████▋ | 2040/2349 [03:12<00:27, 11.38it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  87%|████████▋ | 2048/2349 [03:13<00:26, 11.38it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  88%|████████▊ | 2056/2349 [03:14<00:25, 11.41it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  88%|████████▊ | 2064/2349 [03:15<00:25, 11.12it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  88%|████████▊ | 2072/2349 [03:15<00:25, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  89%|████████▊ | 2080/2349 [03:16<00:24, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  89%|████████▉ | 2088/2349 [03:17<00:24, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  89%|████████▉ | 2096/2349 [03:18<00:23, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  90%|████████▉ | 2104/2349 [03:18<00:22, 10.89it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  90%|████████▉ | 2112/2349 [03:19<00:21, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  90%|█████████ | 2120/2349 [03:20<00:21, 10.66it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  91%|█████████ | 2128/2349 [03:21<00:20, 10.83it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  91%|█████████ | 2136/2349 [03:21<00:19, 10.97it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  91%|█████████▏| 2144/2349 [03:22<00:18, 10.84it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  92%|█████████▏| 2152/2349 [03:23<00:17, 10.98it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  92%|█████████▏| 2160/2349 [03:23<00:17, 11.10it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  92%|█████████▏| 2168/2349 [03:24<00:17, 10.23it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  93%|█████████▎| 2176/2349 [03:25<00:17,  9.64it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  93%|█████████▎| 2184/2349 [03:26<00:17,  9.31it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  93%|█████████▎| 2192/2349 [03:27<00:16,  9.64it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  94%|█████████▎| 2200/2349 [03:28<00:14, 10.34it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  94%|█████████▍| 2208/2349 [03:28<00:13, 10.63it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  94%|█████████▍| 2216/2349 [03:29<00:12, 10.86it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  95%|█████████▍| 2224/2349 [03:30<00:11, 10.77it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  95%|█████████▌| 2232/2349 [03:31<00:11, 10.54it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  95%|█████████▌| 2240/2349 [03:31<00:10, 10.81it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  96%|█████████▌| 2248/2349 [03:32<00:09, 11.02it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  96%|█████████▌| 2256/2349 [03:33<00:08, 11.03it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  96%|█████████▋| 2264/2349 [03:33<00:07, 10.79it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  97%|█████████▋| 2272/2349 [03:34<00:07, 10.13it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  97%|█████████▋| 2280/2349 [03:35<00:07,  9.59it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  97%|█████████▋| 2288/2349 [03:36<00:06,  9.85it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  98%|█████████▊| 2296/2349 [03:37<00:05, 10.26it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  98%|█████████▊| 2304/2349 [03:38<00:04, 10.35it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  98%|█████████▊| 2312/2349 [03:38<00:03, 10.88it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  99%|█████████▉| 2320/2349 [03:39<00:02, 10.78it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  99%|█████████▉| 2328/2349 [03:40<00:01, 10.85it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress:  99%|█████████▉| 2336/2349 [03:40<00:01, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress: 100%|█████████▉| 2344/2349 [03:41<00:00, 10.93it/s]Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "progress: 100%|██████████| 2349/2349 [03:42<00:00, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1081, fp:86, fn:50, tp:1132\n",
      "precision: 0.9293924466338259, recall: 0.9576988155668359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "testdata_path = '/data2/anti_fraud/dataset/test0819.jsonl'\n",
    "model_int4_path = '/data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int4'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_int4_path)\n",
    "model_int4_reload = AutoModelForCausalLM.from_pretrained(model_int4_path, device_map=device)\n",
    "evaluate_with_model(model_int4_reload, tokenizer, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9e3bd-626d-4923-acbe-8bfed04a5f32",
   "metadata": {},
   "source": [
    "tn：1081, fp:86, fn:50, tp:1132\n",
    "precision: 0.9293924466338259, recall: 0.9576988155668359"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b2433-8a95-431b-89bd-e45feb4689e4",
   "metadata": {},
   "source": [
    "> 注：4位量化模型这里之所以要单独加载model，是因为GPTQ量化的4位模型有个限制只能在GPU上运行，我们原先的加载方式会报错，具体参考本文最后的`附：4位量化模型加载错误`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c3db5-0d7b-471d-bf41-4bd267d62ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "从这个结果来看，8位量化与原模型差别不大，但4位量化模型与原始模型的性能差别较大，具体体现在：\n",
    "1. 精确率下降明显，模型在检测欺诈文本时，误报（false positives）数量增加，模型可能会将更多的非欺诈文本错误地分类为欺诈文本。\n",
    "2. 召回率上升，模型在检测欺诈时漏报（false negatives）的数量减少，这意味着模型在检测欺诈文本时更加激进，尽可能减少漏报，哪怕误报增加。\n",
    "\n",
    "4位量化比8位量化引入更多的信息丢失和噪声，模型权重和激活值的精度显著下降，最终导致分类效果的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6c9f7-961f-4b90-812e-580c7382f287",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43d7c002-4627-4645-b591-ae4dd735cb2c",
   "metadata": {},
   "source": [
    "## 模型文件差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69d5c21e-7b4d-47a2-9d0f-b643030e16ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3026376\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua         80 Aug 29 11:30 added_tokens.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua        748 Aug 29 11:30 config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua        242 Aug 29 11:30 generation_config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua    1671853 Aug 29 11:30 merges.txt\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua 1975314632 Aug 29 11:30 model-00001-of-00002.safetensors\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua 1112152304 Aug 29 11:30 model-00002-of-00002.safetensors\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua      27693 Aug 29 11:30 model.safetensors.index.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua        367 Aug 29 11:30 special_tokens_map.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua       1532 Aug 29 11:30 tokenizer_config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua    7028043 Aug 29 11:30 tokenizer.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua    2776833 Aug 29 11:30 vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -l /data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud_1__0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a53e0762-41da-4ca5-b702-5d7dd1685272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2235860\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua         80 Sep 10 11:53 added_tokens.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua       1062 Sep 10 11:53 config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua    1671853 Sep 10 11:53 merges.txt\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua 2278014312 Sep 10 11:53 model.safetensors\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua        269 Sep 10 11:53 quantize_config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua        367 Sep 10 11:53 special_tokens_map.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua       1532 Sep 10 11:53 tokenizer_config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua    7028043 Sep 10 11:53 tokenizer.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua    2776833 Sep 10 11:53 vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -l /data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb2cf47-8d48-4959-a822-8c6b3550c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1591120\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua         80 Sep 10 12:50 added_tokens.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua       1088 Sep 10 18:12 config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua    1671853 Sep 10 12:50 merges.txt\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua 1617798120 Sep 10 12:50 model.safetensors\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua        269 Sep 10 12:50 quantize_config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua        367 Sep 10 12:50 special_tokens_map.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua       1532 Sep 10 12:50 tokenizer_config.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua    7028043 Sep 10 12:50 tokenizer.json\n",
      "-rw-rw-r-- 1 xiaoguanghua xiaoguanghua    2776833 Sep 10 12:50 vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -l /data2/anti_fraud/models/Qwen2-1__5B-Instruct-anti_fraud-gptq-int4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42279381-89bd-4194-a6fc-efed3cd1fe45",
   "metadata": {},
   "source": [
    "可以看到，原始模型、8位量化、4位量化的模型文件大小分别3.08GB、2.27GB、1.61GB，模型文件大小与量化位宽的比例并不完全是线性关系。因为除了模型参数本身之外，还有模型架构、框架开销（pytorch）、优化器的动量和梯度信息等，都会影响着模型文件的总大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9daad-86c6-4b69-a9e7-c297812af77e",
   "metadata": {},
   "source": [
    "## 附：4位量化模型加载错误\n",
    "\n",
    "使用如下代码进行先CPU加载再移到目标GPU时会报`Found modules on cpu/disk`错误：\n",
    "```\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).eval().to(device)\n",
    "```\n",
    "错误详情：\n",
    "```\n",
    "ValueError: Found modules on cpu/disk. Using Exllama or Exllamav2 backend requires all the modules to be on GPU.You can deactivate exllama backend by setting `disable_exllama=True` in the quantization config object\n",
    "```\n",
    "\n",
    "**原因**：使用GPTQ方式量化int4模型时使用了exllama，这是一种高效的kernel实现，但需要所有模型参数在GPU上，因此对于GPTQ的4位量化模型，先使用CPU加载再移到GPU这种做法行不通。\n",
    "\n",
    "**解法**：\n",
    "1. 在模型目录下的config.json文件中，在quantization_config配置块中设置`disable_exllama=true`或者`use_exllama=false`，来禁用exllama，不过可能会影响推理速度。\n",
    "2. 在加载模型时直接加载到GPU上，类似`from_disk = AutoModelForCausalLM.from_pretrained(path, device_map=\"cuda:0\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2b553-134b-4c8f-a831-7ef52876e620",
   "metadata": {},
   "source": [
    "**小结**：本文通过gptq方法分别对微调后的模型进行了8位量化和4位量化，并对比了量化前后模型的性能指标差异，8位量化模型的性能指标变化小，而4位量化模型的性能指标变异较大。就我们这个场景来说，更适合采用8位量化模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2935c-ef54-44b2-a83a-a651903f8b9b",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "- [大模型量化技术原理](https://mp.weixin.qq.com/s/BErKQW5WzFKbZOrUvknT9A)\n",
    "- [使用GPTQ、AWQ、BitsAndBytes量化](https://mp.weixin.qq.com/s/ijTcIuLukD0NR27EFjiJ1Q)\n",
    "- [大模型量化技术前沿](https://mp.weixin.qq.com/s/dZEXUJSioyrk8qL_ykK0mg)\n",
    "- [哪种量化方法适合你](https://blog.csdn.net/wjjc1017/article/details/136274364)\n",
    "- [Found modules on cpu/disk错误讨论](https://github.com/QwenLM/Qwen/issues/385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722a263-912d-47c7-8400-93bb150a1a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
