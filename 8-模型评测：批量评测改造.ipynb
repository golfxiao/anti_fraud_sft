{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e031d2-f510-4cb7-9902-eba0813e2f9b",
   "metadata": {},
   "source": [
    "## 引言\n",
    "\n",
    "前面的模型评测脚本evaluate.py在使用时一直有一个问题，每次评估耗时很长，如下所示，两千条数据需要将近20分钟。\n",
    "`progress: 100%|██████████| 2348/2348 [19:12<00:00,  2.04it/s]`\n",
    "但是根据模型训练时的batch_size设置，模型是支持批量预测的，所以我们有必要根据此特性对评测脚本做一次改造，以支持批量预测，提高每次评测的效率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d243aa-cc30-4fb5-98cd-dec2120bd6f9",
   "metadata": {},
   "source": [
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ae615e-0589-4143-99f2-f6fd3f7969ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f43fe3e-c709-4790-a203-f4557bc769fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_path = '/data2/anti_fraud/dataset/eval0819.jsonl'\n",
    "model_path = '/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct'\n",
    "checkpoint_path = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-900'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346906c-de5d-4441-99ea-621c0464cbe1",
   "metadata": {},
   "source": [
    "加载模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15dd539e-ab7c-4438-b77b-f9d3a08ac5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(model_path, checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6df8df-95c6-46f1-99a3-b00e84257e74",
   "metadata": {},
   "source": [
    "加载数据集，并构造用于测试的小批量数据（8条）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9353d9e-73e8-4d0b-8066-ef9dbb85e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_jsonl(testdata_path)\n",
    "batch_data = dataset[0: 8]\n",
    "contents = [item['input'] for item in batch_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221e09a-ee67-4524-9a92-e84cd1294ab6",
   "metadata": {},
   "source": [
    "## 调试可行性\n",
    "\n",
    "封装一个方法用于构造提示词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1415aa3f-34c2-4751-903c-ca48d9775d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(content):\n",
    "    prompt = f\"下面是一段对话文本, 请分析对话内容是否有诈骗风险，只以json格式输出你的判断结果(is_fraud: true/false)。\\n\\n{content}\"\n",
    "    return [{\"role\": \"user\", \"content\": prompt}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbfe93-75de-49ef-baf2-9adea6439198",
   "metadata": {},
   "source": [
    "按照模型要求的格式来填充提示词。\n",
    "\n",
    "> 注：单条使用apply_chat_template时直接指定了`tokenize=True`和`return_tensors=\"pt\"`参数完成了序列化和张量转换。但批量时则需要使用tokenizer来进行序列化，内部会做一些针对批量的特殊处理（例如填充长度），因此不能指定这两个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b4a673a-3ec7-46be-a0ca-81397bc75fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_chat_complete:<class 'list'>, len: 8, tokenized[0]:<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "下面是一段对话文本, 请分析对话内容是否有诈骗风险，只以json格式输出你的判断结果(is_fraud: true/false)。\n",
      "\n",
      "发言人3: 如果投资一个亿就能回收，并且后面全全都是他的那个效益。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [build_prompt(content) for content in contents]\n",
    "tokenized = tokenizer.apply_chat_template(prompts, add_generation_prompt=True, tokenize=False)\n",
    "print(f\"apply_chat_complete:{type(tokenized)}, len: {len(tokenized)}, tokenized[0]:{tokenized[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71e9ff5-f0b5-4920-a71c-73a23cdeefad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[151644,   8948,    198,  ..., 151643, 151643, 151643],\n",
      "        [151644,   8948,    198,  ..., 151643, 151643, 151643],\n",
      "        [151644,   8948,    198,  ..., 151643, 151643, 151643],\n",
      "        ...,\n",
      "        [151644,   8948,    198,  ..., 151643, 151643, 151643],\n",
      "        [151644,   8948,    198,  ..., 151643, 151643, 151643],\n",
      "        [151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(tokenized, padding=True, return_tensors=\"pt\").to(device)\n",
    "print(f\"inputs: {inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d754545-bef8-408e-a19c-d571715f86df",
   "metadata": {},
   "source": [
    "> 注意这个padding=True的参数，如果不设置此参数，会报```ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).```\n",
    "\n",
    "> 这句话的意思是说：当使用批量模式时，必须将一个批量内的数据长度对齐，对齐方式有两种选择：1）以长的序列为主，将短的序列填充，对应padding参数； 2）以短的序列为主，将长的截断，对应truncation参数；\n",
    "\n",
    "封装一个predict_with_tensors函数，用于批量生成文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020ecd92-6ba8-47aa-94dc-cef5f38d2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_tensors(model, tokenizer, inputs, device='cuda', debug=False):\n",
    "    default_response = {'is_fraud': False}\n",
    "    gen_kwargs = {\"max_new_tokens\": 2048, \"do_sample\": True, \"top_k\": 1}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        responses = []\n",
    "        for i in range(outputs.size(0)):\n",
    "            output = outputs[i, inputs['input_ids'].shape[1]:]\n",
    "            response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            responses.append(safe_loads(response, default_response))\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c097edb6-d3f6-4b85-a974-8ffe2f927a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 1. {\"is_fraud\": false}\n",
      "invalid json: 10,12 {\"is_fraud\": false}\n",
      "invalid json: \n",
      "invalid json: peeches\n",
      "invalid json: 10. is_fraud: true\n",
      "CPU times: user 1.82 s, sys: 272 ms, total: 2.09 s\n",
      "Wall time: 2.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'is_fraud': False},\n",
       " {'is_fraud': False},\n",
       " {'is_fraud': False},\n",
       " {'is_fraud': True},\n",
       " 100,\n",
       " {'is_fraud': False},\n",
       " {'is_fraud': False},\n",
       " {'is_fraud': True}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predict_with_tensors(model, tokenizer, inputs, device, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb375e9-a48b-4e42-9282-cc22b33f09bc",
   "metadata": {},
   "source": [
    "结果出现了好多invalid json错误, 伴随着一些奇怪的输出`10,12 {\"is_fraud\": false}`。\n",
    "\n",
    "仔细看的话，上面有一句警告，意思是说：在使用仅解码器的模型架构时，检测到使用了右侧填充，这种填充方式可能会影响生成结果，建议在初始化tokenizer时将padding_side参数设置为'left'使用左侧填充。\n",
    "\n",
    "再查看上面的inputs，input_ids右侧有很多151643，151643是什么呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe0aa57-2e66-4d97-b9b0-d6ede35edb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064bf67-cb8b-4154-a4db-ea8e077cd40b",
   "metadata": {},
   "source": [
    "由上面的特殊token列表可以看到，151643对应的`<|endoftext|>`在special_tokens中恰好是填充`pad_token`。\n",
    "\n",
    "那句警告正好符合我们的问题现象，那就按照建议重新初始化一个padding_side='left'的tokenizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b209b640-6898-4651-a6c1-d42a11302d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_left = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e673bd-940c-47ca-94f8-f04de1019883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
      "        [151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
      "        [151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
      "        ...,\n",
      "        [151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
      "        [151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
      "        [151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_left(tokenized, padding=True, return_tensors=\"pt\").to(device)\n",
    "print(f\"inputs: {inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372528e4-58bd-4369-ac9d-8fffaad28da5",
   "metadata": {},
   "source": [
    "这次的序列化结果中，151643(填充token)都到了左边，看起来是对了。\n",
    "\n",
    "尝试反序列化一下，看是否正常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bf401b7-e3b3-49f0-ad74-cf55b16d0ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nYou are a helpful assistant.\\nuser\\n下面是一段对话文本, 请分析对话内容是否有诈骗风险，只以json格式输出你的判断结果(is_fraud: true/false)。\\n\\n发言人3: 如果投资一个亿就能回收，并且后面全全都是他的那个效益。\\nassistant\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_left.decode(inputs['input_ids'][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3bb41-bda2-4447-b2c9-98130f87a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "再次运行批量预测函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ed665e9-7911-4de0-9d21-fa030ee216e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 615 ms, sys: 43 ms, total: 658 ms\n",
      "Wall time: 655 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'is_fraud': False},\n",
       " {'is_fraud': False},\n",
       " {'is_fraud': False},\n",
       " {'is_fraud': True},\n",
       " {'is_fraud': True},\n",
       " {'is_fraud': False},\n",
       " {'is_fraud': True},\n",
       " {'is_fraud': True}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predict_with_tensors(model, tokenizer, inputs, device, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00914936-47ea-4281-98c9-7e901ced9b1d",
   "metadata": {},
   "source": [
    "## 脚本改造\n",
    "\n",
    "封装一个真正的predict_batch函数用于支持批量预测，并同步添加一个run_test_batch函数也用于支持批量解析标签，最后都添加到evaluate.py脚本中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6fe27d-81b8-4314-9da7-01c2b8b07950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, tokenizer, contents: List[str], device='cuda', debug=False):\n",
    "    prompts = [build_prompt(content) for content in contents]\n",
    "    inputs = tokenizer(\n",
    "        tokenizer.apply_chat_template(prompts, add_generation_prompt=True, tokenize=False),\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    default_response = {'is_fraud': False}\n",
    "    gen_kwargs = {\"max_new_tokens\": 2048, \"do_sample\": True, \"top_k\": 1}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        responses = []\n",
    "        for i in range(outputs.size(0)):\n",
    "            output = outputs[i, inputs['input_ids'].shape[1]:]\n",
    "            response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            responses.append(safe_loads(response, default_response))\n",
    "        return responses\n",
    "\n",
    "def run_test_batch(model, tokenizer, test_data: List[Dict], batch_size: int = 8, device='cuda', debug=False):\n",
    "    print(f\"run in batch mode, batch_size={batch_size}\")\n",
    "    real_labels = []\n",
    "    pred_labels = []\n",
    "    pbar = tqdm(total=len(test_data), desc=f'progress')\n",
    "    \n",
    "    for i in range(0, len(test_data), batch_size):\n",
    "        batch_data = test_data[i:i + batch_size]\n",
    "        dialog_inputs = [item['input'] for item in batch_data]\n",
    "        real_batch_labels = [item['label'] for item in batch_data]\n",
    "        \n",
    "        predictions = predict_batch(model, tokenizer, dialog_inputs, device)\n",
    "        pred_batch_labels = [prediction['is_fraud'] for prediction in predictions]\n",
    "        \n",
    "        real_labels.extend(real_batch_labels)\n",
    "        pred_labels.extend(pred_batch_labels)\n",
    "        \n",
    "        pbar.update(len(batch_data))\n",
    "        \n",
    "    return real_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7436f-d688-4cb1-a2a4-800c48bfa42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "再对evaluate函数稍作改造，扩展一个batch=true/false参数来支持批量评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e809c98-7bc8-4d67-b8b9-f2ff55882e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_model(model, tokenizer, testdata_path, device='cuda', batch=False, debug=False):\n",
    "    dataset = load_jsonl(testdata_path)\n",
    "    run_test_func = run_test_batch if batch else run_test\n",
    "    true_labels, pred_labels = run_test_func(model, tokenizer, dataset, device=device, debug=debug)\n",
    "    precision, recall = precision_recall(true_labels, pred_labels, debug=debug)\n",
    "    print(f\"precision: {precision}, recall: {recall}\")\n",
    "\n",
    "def evaluate(model_path, checkpoint_path, testdata_path, device='cuda', batch=False, debug=False):    \n",
    "    model, tokenizer = load_model(model_path, checkpoint_path, device)\n",
    "    evaluate_with_model(model, tokenizer, testdata_path, device, batch, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fae759-807f-4e21-b340-56271ab11e2e",
   "metadata": {},
   "source": [
    "## 批量评估测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fefb3554-86e5-4ba1-a17a-cc0f31ac75c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:   0%|          | 0/2348 [05:08<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:14<00:00, 12.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1160, fp:5, fn:103, tp:1080\n",
      "precision: 0.9953917050691244, recall: 0.9129332206255283\n",
      "CPU times: user 3min 16s, sys: 21.9 s, total: 3min 38s\n",
      "Wall time: 3min 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(model_path, checkpoint_path, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f02af5-a4a7-4b22-b8a9-f6b8a47ffd50",
   "metadata": {},
   "source": [
    "同样的数据量（2348条），之前单条模式需要`19min 12s`,而批量模式只需要`3min 16s`，耗时只有原来的1/6。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b9f33f3-c0a9-4074-863e-d98d45bc93a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run in batch mode, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:12<00:00, 12.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1160, fp:5, fn:89, tp:1094\n",
      "precision: 0.9954504094631483, recall: 0.9247675401521556\n",
      "CPU times: user 3min 15s, sys: 21.8 s, total: 3min 37s\n",
      "Wall time: 3min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "checkpoint_path_1200 = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0826/checkpoint-1200'\n",
    "evaluate(model_path, checkpoint_path_1200, testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf17de-e2e7-4162-b265-04bac80dc281",
   "metadata": {},
   "source": [
    "**小结**：本文通过对模型批量生成文本的探索，让评测函数支持了批量预测，提高了评测效率的同时，也对padding的使用场景以及左、右填充的区别有了更深的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5a785-e3ab-4048-aa05-8e97624fbae2",
   "metadata": {},
   "source": [
    "## 相关阅读\n",
    "-  [欺诈文本分类微调（五）：模型评测](https://golfxiao.blog.csdn.net/article/details/141355995)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
